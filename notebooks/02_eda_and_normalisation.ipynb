{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d73995",
   "metadata": {},
   "source": [
    "# Notebook 02: EDA and Normalisation\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) and light normalisation on the document level dataset produced in Notebook 01.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "This notebook expects the following processed files from Notebook 01:\n",
    "\n",
    "- `data/processed/docs_extracted.csv`\n",
    "- `data/processed/docs_extracted.json` (used for inspection if needed)\n",
    "\n",
    "Each row in `docs_extracted.csv` represents a single synthetic insurance related document with fields such as:\n",
    "\n",
    "- `entity_name`\n",
    "- `region`\n",
    "- `sector`\n",
    "- `risk_type`\n",
    "- `time_horizon`\n",
    "- `key_risk_factors`\n",
    "- `risk_summary`\n",
    "- document level metadata (for example filename and category)\n",
    "\n",
    "## Goals of this notebook\n",
    "\n",
    "The main goals are to:\n",
    "\n",
    "1. Load the extracted document level dataset into pandas.\n",
    "2. Perform high level EDA on the structured fields:\n",
    "   - basic distributions for `region`, `sector`, `risk_type`, `time_horizon`\n",
    "   - counts and patterns by document category (policies, esg, incidents)\n",
    "3. Inspect missingness and simple data quality issues.\n",
    "4. Analyse text lengths:\n",
    "   - length of `risk_summary` in characters and words\n",
    "   - simple checks on the number of `key_risk_factors` per document\n",
    "5. Create a small set of derived numeric features:\n",
    "   - `num_risk_factors`\n",
    "   - `summary_length_words`\n",
    "   - `summary_length_chars`\n",
    "6. Apply light normalisation to categorical fields where needed (for example consistent naming and ordering).\n",
    "\n",
    "The focus here is understanding the dataset and preparing clean features, not heavy cleaning or modelling.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "At the end of this notebook we will produce:\n",
    "\n",
    "- A clean document level DataFrame ready for modelling.\n",
    "- A saved processed file:\n",
    "\n",
    "  - `data/processed/docs_clean.csv`\n",
    "\n",
    "This will be the main input to Notebook 03, where we will build and evaluate a simple High Risk vs Low Risk classifier on top of these features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6a267",
   "metadata": {},
   "source": [
    "## Step 1: Load the extracted document level dataset\n",
    "\n",
    "In this step we load the output of Notebook 01, which is the file `docs_extracted.csv` stored in the `data/processed/` folder.\n",
    "\n",
    "This dataset contains one row per document with fields extracted by the LLM, including:\n",
    "\n",
    "- categorical fields such as `region`, `sector`, `risk_type`, `time_horizon`\n",
    "- text fields such as `risk_summary`\n",
    "- list based fields such as `key_risk_factors`\n",
    "\n",
    "Once loaded, we will inspect the basic structure of the DataFrame to confirm:\n",
    "\n",
    "- the number of documents\n",
    "- column names\n",
    "- presence of any missing values\n",
    "- a quick preview of the first few rows\n",
    "\n",
    "This gives us a consistent starting point for further EDA in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40435ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of docs_df: (14, 12)\n",
      "\n",
      "Column names:\n",
      "['entity_name', 'region', 'sector', 'risk_type', 'time_horizon', 'key_risk_factors', 'risk_summary', 'doc_index', 'category', 'filename', 'n_chunks', 'n_chunks_with_data']\n",
      "\n",
      "Data types and non null counts:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   entity_name         10 non-null     object\n",
      " 1   region              14 non-null     object\n",
      " 2   sector              14 non-null     object\n",
      " 3   risk_type           14 non-null     object\n",
      " 4   time_horizon        14 non-null     object\n",
      " 5   key_risk_factors    14 non-null     object\n",
      " 6   risk_summary        14 non-null     object\n",
      " 7   doc_index           14 non-null     int64 \n",
      " 8   category            14 non-null     object\n",
      " 9   filename            14 non-null     object\n",
      " 10  n_chunks            14 non-null     int64 \n",
      " 11  n_chunks_with_data  14 non-null     int64 \n",
      "dtypes: int64(3), object(9)\n",
      "memory usage: 1.4+ KB\n",
      "None\n",
      "\n",
      "Preview of the first few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>region</th>\n",
       "      <th>sector</th>\n",
       "      <th>risk_type</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>key_risk_factors</th>\n",
       "      <th>risk_summary</th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>n_chunks_with_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global manufacturing, logistics, and energy co...</td>\n",
       "      <td>global</td>\n",
       "      <td>industrial</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['inconsistent ESG strategy', 'different inter...</td>\n",
       "      <td>The company's ESG strategy is still developing...</td>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The organisation</td>\n",
       "      <td>global</td>\n",
       "      <td>energy, logistics, heavy industry, transport</td>\n",
       "      <td>esg</td>\n",
       "      <td>medium_term</td>\n",
       "      <td>['inconsistency in language used in internal d...</td>\n",
       "      <td>The organisation's energy transition strategy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "      <td>global</td>\n",
       "      <td>energy</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['inconsistent expectations in environmental c...</td>\n",
       "      <td>The company's supply chain faces risks related...</td>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>global</td>\n",
       "      <td>marine</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['engine failure', 'grounding', 'irregular vib...</td>\n",
       "      <td>The incident involved a synthetic marine vesse...</td>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Motor Fleet Collision Event</td>\n",
       "      <td>global</td>\n",
       "      <td>transportation</td>\n",
       "      <td>motor</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['driver error', 'vehicle maintenance', 'senso...</td>\n",
       "      <td>A motor fleet collision event occurred on a du...</td>\n",
       "      <td>4</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_motor_fleet_collision.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         entity_name  region  \\\n",
       "0  global manufacturing, logistics, and energy co...  global   \n",
       "1                                   The organisation  global   \n",
       "2  Synthetic ESG Report – Supply Chain and Climat...  global   \n",
       "3                                                NaN  global   \n",
       "4                        Motor Fleet Collision Event  global   \n",
       "\n",
       "                                         sector risk_type   time_horizon  \\\n",
       "0                                    industrial       esg  not_specified   \n",
       "1  energy, logistics, heavy industry, transport       esg    medium_term   \n",
       "2                                        energy       esg  not_specified   \n",
       "3                                        marine  property  not_specified   \n",
       "4                                transportation     motor  not_specified   \n",
       "\n",
       "                                    key_risk_factors  \\\n",
       "0  ['inconsistent ESG strategy', 'different inter...   \n",
       "1  ['inconsistency in language used in internal d...   \n",
       "2  ['inconsistent expectations in environmental c...   \n",
       "3  ['engine failure', 'grounding', 'irregular vib...   \n",
       "4  ['driver error', 'vehicle maintenance', 'senso...   \n",
       "\n",
       "                                        risk_summary  doc_index  category  \\\n",
       "0  The company's ESG strategy is still developing...          0       esg   \n",
       "1  The organisation's energy transition strategy ...          1       esg   \n",
       "2  The company's supply chain faces risks related...          2       esg   \n",
       "3  The incident involved a synthetic marine vesse...          3  incident   \n",
       "4  A motor fleet collision event occurred on a du...          4  incident   \n",
       "\n",
       "                             filename  n_chunks  n_chunks_with_data  \n",
       "0    esg_corporate_sustainability.txt         4                   4  \n",
       "1           esg_energy_transition.txt         5                   5  \n",
       "2     esg_supply_chain_governance.txt         5                   5  \n",
       "3       incident_marine_grounding.txt         4                   4  \n",
       "4  incident_motor_fleet_collision.txt         4                   4  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the extracted document level dataset\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define project root and processed data directory\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Path to the extracted document level CSV from Notebook 01\n",
    "DOCS_EXTRACTED_PATH = DATA_PROCESSED_DIR / \"docs_extracted.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "docs_df = pd.read_csv(DOCS_EXTRACTED_PATH)\n",
    "\n",
    "# Basic checks\n",
    "print(\"Shape of docs_df:\", docs_df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(docs_df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types and non null counts:\")\n",
    "print(docs_df.info())\n",
    "\n",
    "print(\"\\nPreview of the first few rows:\")\n",
    "docs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6022d36",
   "metadata": {},
   "source": [
    "## Step 2: High level overview and basic distributions\n",
    "\n",
    "In this step we take a first look at the structure of the dataset to understand how information is spread across the 14 documents. This involves simple exploratory checks such as:\n",
    "\n",
    "- counts of documents per category (policy, esg, incident)\n",
    "- frequency of values in key extracted fields:\n",
    "  - `region`\n",
    "  - `sector`\n",
    "  - `risk_type`\n",
    "  - `time_horizon`\n",
    "\n",
    "These checks help verify that the extraction pipeline produced reasonable variation and that there are no unexpected values. They also give an initial sense of the dataset before we examine missingness, text lengths, and risk factor patterns in later steps.\n",
    "\n",
    "This step focuses only on simple counts and distributions to build an intuition for the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413ee55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document counts by category:\n",
      "\n",
      "category\n",
      "policy      8\n",
      "esg         3\n",
      "incident    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relative frequencies by category:\n",
      "\n",
      "category\n",
      "policy      0.571429\n",
      "esg         0.214286\n",
      "incident    0.214286\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Region value counts:\n",
      "\n",
      "region\n",
      "global           11\n",
      "north_america     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Region relative frequencies:\n",
      "\n",
      "region\n",
      "global           0.785714\n",
      "north_america    0.214286\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sector value counts:\n",
      "\n",
      "sector\n",
      "insurance                                       6\n",
      "travel                                          2\n",
      "industrial                                      1\n",
      "energy, logistics, heavy industry, transport    1\n",
      "marine                                          1\n",
      "energy                                          1\n",
      "manufacturing                                   1\n",
      "transportation                                  1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sector relative frequencies:\n",
      "\n",
      "sector\n",
      "insurance                                       0.428571\n",
      "travel                                          0.142857\n",
      "industrial                                      0.071429\n",
      "energy, logistics, heavy industry, transport    0.071429\n",
      "marine                                          0.071429\n",
      "energy                                          0.071429\n",
      "manufacturing                                   0.071429\n",
      "transportation                                  0.071429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Risk type value counts:\n",
      "\n",
      "risk_type\n",
      "property     4\n",
      "esg          3\n",
      "liability    3\n",
      "travel       2\n",
      "motor        1\n",
      "cyber        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Risk type relative frequencies:\n",
      "\n",
      "risk_type\n",
      "property     0.285714\n",
      "esg          0.214286\n",
      "liability    0.214286\n",
      "travel       0.142857\n",
      "motor        0.071429\n",
      "cyber        0.071429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Time horizon value counts:\n",
      "\n",
      "time_horizon\n",
      "not_specified    11\n",
      "short_term        2\n",
      "medium_term       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time horizon relative frequencies:\n",
      "\n",
      "time_horizon\n",
      "not_specified    0.785714\n",
      "short_term       0.142857\n",
      "medium_term      0.071429\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: High level overview and basic distributions\n",
    "\n",
    "# 1. Count documents per category (policy, esg, incident)\n",
    "print(\"Document counts by category:\\n\")\n",
    "print(docs_df[\"category\"].value_counts())\n",
    "print(\"\\nRelative frequencies by category:\\n\")\n",
    "print(docs_df[\"category\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 2. Frequency of regions\n",
    "print(\"Region value counts:\\n\")\n",
    "print(docs_df[\"region\"].value_counts())\n",
    "print(\"\\nRegion relative frequencies:\\n\")\n",
    "print(docs_df[\"region\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 3. Frequency of sectors\n",
    "print(\"Sector value counts:\\n\")\n",
    "print(docs_df[\"sector\"].value_counts())\n",
    "print(\"\\nSector relative frequencies:\\n\")\n",
    "print(docs_df[\"sector\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 4. Frequency of risk types\n",
    "print(\"Risk type value counts:\\n\")\n",
    "print(docs_df[\"risk_type\"].value_counts())\n",
    "print(\"\\nRisk type relative frequencies:\\n\")\n",
    "print(docs_df[\"risk_type\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 5. Frequency of time horizons\n",
    "print(\"Time horizon value counts:\\n\")\n",
    "print(docs_df[\"time_horizon\"].value_counts())\n",
    "print(\"\\nTime horizon relative frequencies:\\n\")\n",
    "print(docs_df[\"time_horizon\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcce92c",
   "metadata": {},
   "source": [
    "## Step 3: Missingness and data quality checks\n",
    "\n",
    "In this step we examine the dataset for missing values and potential data quality issues. The goal is to understand which fields are reliable, which fields need light normalisation, and which fields should not be used directly without processing.\n",
    "\n",
    "Key checks in this step:\n",
    "\n",
    "- Identify columns with missing values, especially `entity_name`\n",
    "- Inspect whether list based fields such as `key_risk_factors` have been stored as strings\n",
    "- Examine the consistency and completeness of the `risk_summary` text\n",
    "- Check for placeholder or default values such as `global` in `region` or `not_specified` in `time_horizon`\n",
    "- Look for unusual or unexpected categories that may need correction\n",
    "\n",
    "These checks help us decide how much cleaning or normalisation is needed before moving to feature engineering. The objective is not heavy cleaning, but building a clear understanding of the strengths and weaknesses of each field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e073f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "\n",
      "entity_name           4\n",
      "region                0\n",
      "sector                0\n",
      "risk_type             0\n",
      "time_horizon          0\n",
      "key_risk_factors      0\n",
      "risk_summary          0\n",
      "doc_index             0\n",
      "category              0\n",
      "filename              0\n",
      "n_chunks              0\n",
      "n_chunks_with_data    0\n",
      "dtype: int64\n",
      "\n",
      "Proportion missing per column:\n",
      "\n",
      "entity_name           0.285714\n",
      "region                0.000000\n",
      "sector                0.000000\n",
      "risk_type             0.000000\n",
      "time_horizon          0.000000\n",
      "key_risk_factors      0.000000\n",
      "risk_summary          0.000000\n",
      "doc_index             0.000000\n",
      "category              0.000000\n",
      "filename              0.000000\n",
      "n_chunks              0.000000\n",
      "n_chunks_with_data    0.000000\n",
      "dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Value counts for region:\n",
      "\n",
      "region\n",
      "global           11\n",
      "north_america     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relative frequencies:\n",
      "\n",
      "region\n",
      "global           0.785714\n",
      "north_america    0.214286\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Value counts for time_horizon:\n",
      "\n",
      "time_horizon\n",
      "not_specified    11\n",
      "short_term        2\n",
      "medium_term       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relative frequencies:\n",
      "\n",
      "time_horizon\n",
      "not_specified    0.785714\n",
      "short_term       0.142857\n",
      "medium_term      0.071429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Sample entity_name values (including NaNs):\n",
      "\n",
      "0    global manufacturing, logistics, and energy co...\n",
      "1                                     The organisation\n",
      "2    Synthetic ESG Report – Supply Chain and Climat...\n",
      "3                                                  NaN\n",
      "4                          Motor Fleet Collision Event\n",
      "5                                                  NaN\n",
      "6                             Synthetic Auto Insurance\n",
      "7                                                  NaN\n",
      "8                                      Cyber Insurance\n",
      "9                       Synthetic Group Life Insurance\n",
      "Name: entity_name, dtype: object\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Type of a key_risk_factors entry: <class 'str'>\n",
      "\n",
      "Sample key_risk_factors values:\n",
      "\n",
      "0    ['inconsistent ESG strategy', 'different inter...\n",
      "1    ['inconsistency in language used in internal d...\n",
      "2    ['inconsistent expectations in environmental c...\n",
      "3    ['engine failure', 'grounding', 'irregular vib...\n",
      "4    ['driver error', 'vehicle maintenance', 'senso...\n",
      "Name: key_risk_factors, dtype: object\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Risk summary length stats (characters):\n",
      "\n",
      "count     14.000000\n",
      "mean     800.285714\n",
      "std       13.612373\n",
      "min      753.000000\n",
      "25%      804.000000\n",
      "50%      804.000000\n",
      "75%      804.000000\n",
      "max      804.000000\n",
      "Name: risk_summary, dtype: float64\n",
      "\n",
      "Sample risk_summary values:\n",
      "\n",
      "\n",
      "Document 0, length 804 characters:\n",
      "\n",
      "The company's ESG strategy is still developing and not consistently implemented across business units, leading to an inconsistent picture when presenting a single global ESG narrative. Different regio ...\n",
      "\n",
      "Document 1, length 804 characters:\n",
      "\n",
      "The organisation's energy transition strategy is hindered by inconsistency in language and difficulty in determining actual plans versus ideas. There are concerns about the accuracy of reported renewa ...\n",
      "\n",
      "Document 2, length 804 characters:\n",
      "\n",
      "The company's supply chain faces risks related to inconsistent environmental expectations, incomplete documentation, and data reliability issues, which may impact sustainability goals. The company fac ...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Missingness and data quality checks\n",
    "\n",
    "# 1. Missing values per column\n",
    "print(\"Missing values per column:\\n\")\n",
    "missing_counts = docs_df.isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "print(\"\\nProportion missing per column:\\n\")\n",
    "missing_props = docs_df.isna().mean()\n",
    "print(missing_props)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 2. Inspect placeholder style values in key extracted fields\n",
    "for col in [\"region\", \"time_horizon\"]:\n",
    "    print(f\"Value counts for {col}:\\n\")\n",
    "    print(docs_df[col].value_counts())\n",
    "    print(\"\\nRelative frequencies:\\n\")\n",
    "    print(docs_df[col].value_counts(normalize=True))\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 3. Inspect entity_name values, including missing ones\n",
    "print(\"Sample entity_name values (including NaNs):\\n\")\n",
    "print(docs_df[\"entity_name\"].head(10))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 4. Inspect key_risk_factors format\n",
    "print(\"Type of a key_risk_factors entry:\", type(docs_df.loc[0, \"key_risk_factors\"]))\n",
    "print(\"\\nSample key_risk_factors values:\\n\")\n",
    "print(docs_df[\"key_risk_factors\"].head(5))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 5. Basic properties of risk_summary\n",
    "summary_lengths = docs_df[\"risk_summary\"].str.len()\n",
    "\n",
    "print(\"Risk summary length stats (characters):\\n\")\n",
    "print(summary_lengths.describe())\n",
    "\n",
    "print(\"\\nSample risk_summary values:\\n\")\n",
    "for i, text in docs_df[\"risk_summary\"].head(3).items():\n",
    "    print(f\"\\nDocument {i}, length {len(text)} characters:\\n\")\n",
    "    print(text[:200], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb27f2f",
   "metadata": {},
   "source": [
    "## Step 4: Convert fields and create early derived features\n",
    "\n",
    "Before moving into deeper EDA, we need to convert a few columns into cleaner formats and create some simple numeric features that will support modelling later.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Convert the `key_risk_factors` column into a real Python list.  \n",
    "   The values in this column come from LLM extraction and are stored as long, irregular strings.  \n",
    "   Instead of using strict parsing, we use a simple and robust regex based approach that:\n",
    "   - extracts every substring inside single quotes `'...'`\n",
    "   - removes any extra whitespace\n",
    "   - always returns a valid Python list  \n",
    "   This gives us a clean list of risk factor phrases for each document.\n",
    "\n",
    "2. Create basic derived numeric features:\n",
    "   - `num_risk_factors`: number of extracted risk factor phrases  \n",
    "   - `summary_length_chars`: character length of the risk_summary text  \n",
    "   - `summary_length_words`: word count of the risk_summary text\n",
    "\n",
    "These early features capture a sense of document complexity and variation across the dataset.  \n",
    "They will be useful both for EDA in this notebook and for model building in Notebook 03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edd9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of derived features:\n",
      "\n",
      "   doc_index  category risk_type  num_risk_factors  summary_length_chars  \\\n",
      "0          0       esg       esg                28                   804   \n",
      "1          1       esg       esg                20                   804   \n",
      "2          2       esg       esg                19                   804   \n",
      "3          3  incident  property                21                   804   \n",
      "4          4  incident     motor                14                   804   \n",
      "\n",
      "   summary_length_words  \n",
      "0                   115  \n",
      "1                   108  \n",
      "2                   106  \n",
      "3                   117  \n",
      "4                   128  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [inconsistent ESG strategy, different interpre...\n",
       "1    [inconsistency in language used in internal do...\n",
       "2    [inconsistent expectations in environmental cr...\n",
       "3    [engine failure, grounding, irregular vibratio...\n",
       "4    [driver error, vehicle maintenance, sensor cal...\n",
       "5    [faulty fire alarm panel, unusual smell, contr...\n",
       "6    [bodily injury, property damage, liability, fa...\n",
       "7    [exclusions, expected losses, expected or inte...\n",
       "8    [malicious attacks, accidental system failures...\n",
       "9    [discrepancies in reported earnings, late enro...\n",
       "Name: key_risk_factors_list, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Convert fields and create early derived features\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Robust parser for key_risk_factors\n",
    "def extract_risk_factors(value):\n",
    "    \"\"\"\n",
    "    Convert the key_risk_factors string into a clean Python list.\n",
    "    Uses regex to extract everything between single quotes.\n",
    "    Always returns a Python list (possibly empty).\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    \n",
    "    text = str(value)\n",
    "    \n",
    "    # Extract all substrings inside single quotes '...'\n",
    "    items = re.findall(r\"'([^']+)'\", text)\n",
    "    \n",
    "    # Strip extra whitespace and return\n",
    "    return [item.strip() for item in items]\n",
    "\n",
    "# Apply parser to create a clean list column\n",
    "docs_df[\"key_risk_factors_list\"] = docs_df[\"key_risk_factors\"].apply(extract_risk_factors)\n",
    "\n",
    "# 2. Create derived numeric features\n",
    "\n",
    "# Number of key risk factors\n",
    "docs_df[\"num_risk_factors\"] = docs_df[\"key_risk_factors_list\"].apply(len)\n",
    "\n",
    "# Length of risk_summary in characters\n",
    "docs_df[\"summary_length_chars\"] = docs_df[\"risk_summary\"].str.len()\n",
    "\n",
    "# Length of risk_summary in words\n",
    "docs_df[\"summary_length_words\"] = docs_df[\"risk_summary\"].str.split().str.len()\n",
    "\n",
    "# Quick check of the new columns\n",
    "print(\"Preview of derived features:\\n\")\n",
    "print(\n",
    "    docs_df[\n",
    "        [\n",
    "            \"doc_index\",\n",
    "            \"category\",\n",
    "            \"risk_type\",\n",
    "            \"num_risk_factors\",\n",
    "            \"summary_length_chars\",\n",
    "            \"summary_length_words\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "docs_df[\"key_risk_factors_list\"].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d602bb7",
   "metadata": {},
   "source": [
    "## Step 5: Explore derived features and relationships\n",
    "\n",
    "In this step we use the new derived features to deepen our understanding of the documents and how risk information is distributed.\n",
    "\n",
    "We will focus on:\n",
    "\n",
    "1. Distributions of the derived numeric features:\n",
    "   - `num_risk_factors`\n",
    "   - `summary_length_chars`\n",
    "   - `summary_length_words`\n",
    "\n",
    "2. Simple relationships between these features and key categorical fields:\n",
    "   - by `category` (policy, esg, incident)\n",
    "   - by `risk_type` (property, motor, cyber, travel, liability, esg)\n",
    "\n",
    "Examples of questions we want to answer:\n",
    "\n",
    "- Do incident or ESG style documents tend to have more key risk factors than policies?\n",
    "- Are certain risk types associated with longer summaries or more risk factors?\n",
    "- Is there enough variation in these features to be useful later in modelling?\n",
    "\n",
    "We will use simple descriptive statistics and grouped summaries (for example `groupby().agg(...)`) to build intuition before moving on to normalisation and final cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4df14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of numeric derived features:\n",
      "\n",
      "       num_risk_factors  summary_length_chars  summary_length_words\n",
      "count         14.000000             14.000000             14.000000\n",
      "mean          21.000000            800.285714            112.642857\n",
      "std            7.421383             13.612373              7.438022\n",
      "min           11.000000            753.000000            102.000000\n",
      "25%           14.250000            804.000000            108.000000\n",
      "50%           20.000000            804.000000            112.500000\n",
      "75%           25.000000            804.000000            116.500000\n",
      "max           36.000000            804.000000            128.000000\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Derived feature means by category:\n",
      "\n",
      "          num_risk_factors  summary_length_chars  summary_length_words\n",
      "category                                                              \n",
      "esg              22.333333                 804.0            109.666667\n",
      "incident         15.333333                 804.0            121.333333\n",
      "policy           22.625000                 797.5            110.500000\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Derived feature means by risk_type:\n",
      "\n",
      "           num_risk_factors  summary_length_chars  summary_length_words\n",
      "risk_type                                                              \n",
      "cyber             36.000000            804.000000            110.000000\n",
      "esg               22.333333            804.000000            109.666667\n",
      "liability         17.666667            803.666667            115.000000\n",
      "motor             14.000000            804.000000            128.000000\n",
      "property          19.750000            791.250000            113.000000\n",
      "travel            22.500000            804.000000            106.500000\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Quick comparison table of key fields:\n",
      "\n",
      "    category  risk_type  num_risk_factors  summary_length_words\n",
      "8     policy      cyber                36                   110\n",
      "11    policy   property                32                   114\n",
      "0        esg        esg                28                   115\n",
      "12    policy     travel                25                   111\n",
      "6     policy  liability                25                   108\n",
      "3   incident   property                21                   117\n",
      "13    policy     travel                20                   102\n",
      "1        esg        esg                20                   108\n",
      "2        esg        esg                19                   106\n",
      "10    policy   property                15                   102\n",
      "9     policy  liability                14                   115\n",
      "4   incident      motor                14                   128\n",
      "7     policy  liability                14                   122\n",
      "5   incident   property                11                   119\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Explore derived features and relationships\n",
    "\n",
    "# 1. Distribution of numeric features\n",
    "print(\"Summary of numeric derived features:\\n\")\n",
    "print(\n",
    "    docs_df[\n",
    "        [\"num_risk_factors\", \"summary_length_chars\", \"summary_length_words\"]\n",
    "    ].describe()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 2. Compare derived features by category (policy, esg, incident)\n",
    "print(\"Derived feature means by category:\\n\")\n",
    "print(\n",
    "    docs_df.groupby(\"category\")[\n",
    "        [\"num_risk_factors\", \"summary_length_chars\", \"summary_length_words\"]\n",
    "    ].mean()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 3. Compare derived features by risk_type\n",
    "print(\"Derived feature means by risk_type:\\n\")\n",
    "print(\n",
    "    docs_df.groupby(\"risk_type\")[\n",
    "        [\"num_risk_factors\", \"summary_length_chars\", \"summary_length_words\"]\n",
    "    ].mean()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# 4. Optional: show variation with simple scatter style table\n",
    "print(\"Quick comparison table of key fields:\\n\")\n",
    "print(\n",
    "    docs_df[\n",
    "        [\"category\", \"risk_type\", \"num_risk_factors\", \"summary_length_words\"]\n",
    "    ].sort_values(\"num_risk_factors\", ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b81754",
   "metadata": {},
   "source": [
    "## Step 6: Light normalisation and final cleaning\n",
    "\n",
    "Before saving the cleaned dataset, we apply a small number of normalisation steps to improve consistency and prepare the data for modelling in Notebook 03.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Standardise categorical fields where simple inconsistencies appear  \n",
    "   (for example lowercasing or trimming whitespace if needed).\n",
    "\n",
    "2. Ensure that controlled vocabulary fields such as  \n",
    "   `risk_type`, `region`, and `time_horizon` follow the same spelling and format  \n",
    "   used during extraction.\n",
    "\n",
    "3. Inspect the `sector` field for minor inconsistencies and decide whether  \n",
    "   to normalise or keep as-is.  \n",
    "   Since sectors vary widely across documents, we will avoid heavy cleaning  \n",
    "   and keep the original labels unless there are obvious duplicates.\n",
    "\n",
    "4. Remove or reorder columns that are not useful for modelling  \n",
    "   (for example internal metadata such as `filename` or `n_chunks`).\n",
    "\n",
    "5. Prepare the final set of columns to be saved as  \n",
    "   `data/processed/docs_clean.csv`, which will be the main input to  \n",
    "   Notebook 03 for feature engineering and classification.\n",
    "\n",
    "This step focuses on minimal, safe adjustments rather than aggressive cleaning,  \n",
    "since the dataset is small and the extraction pipeline already enforces structure.  \n",
    "The goal is simply to ensure consistency and produce a tidy DataFrame ready for modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af041ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset to: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\processed\\docs_clean.csv\n",
      "\n",
      "Shape of docs_clean: (14, 12)\n",
      "\n",
      "Columns in docs_clean:\n",
      " ['doc_index', 'category', 'entity_name', 'region', 'sector', 'risk_type', 'time_horizon', 'key_risk_factors_list', 'num_risk_factors', 'summary_length_chars', 'summary_length_words', 'risk_summary']\n",
      "\n",
      "Preview of docs_clean:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>region</th>\n",
       "      <th>sector</th>\n",
       "      <th>risk_type</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>key_risk_factors_list</th>\n",
       "      <th>num_risk_factors</th>\n",
       "      <th>summary_length_chars</th>\n",
       "      <th>summary_length_words</th>\n",
       "      <th>risk_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>global manufacturing, logistics, and energy co...</td>\n",
       "      <td>global</td>\n",
       "      <td>industrial</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[inconsistent ESG strategy, different interpre...</td>\n",
       "      <td>28</td>\n",
       "      <td>804</td>\n",
       "      <td>115</td>\n",
       "      <td>The company's ESG strategy is still developing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>The organisation</td>\n",
       "      <td>global</td>\n",
       "      <td>energy, logistics, heavy industry, transport</td>\n",
       "      <td>esg</td>\n",
       "      <td>medium_term</td>\n",
       "      <td>[inconsistency in language used in internal do...</td>\n",
       "      <td>20</td>\n",
       "      <td>804</td>\n",
       "      <td>108</td>\n",
       "      <td>The organisation's energy transition strategy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "      <td>global</td>\n",
       "      <td>energy</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[inconsistent expectations in environmental cr...</td>\n",
       "      <td>19</td>\n",
       "      <td>804</td>\n",
       "      <td>106</td>\n",
       "      <td>The company's supply chain faces risks related...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>unspecified_entity</td>\n",
       "      <td>global</td>\n",
       "      <td>marine</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[engine failure, grounding, irregular vibratio...</td>\n",
       "      <td>21</td>\n",
       "      <td>804</td>\n",
       "      <td>117</td>\n",
       "      <td>The incident involved a synthetic marine vesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>incident</td>\n",
       "      <td>Motor Fleet Collision Event</td>\n",
       "      <td>global</td>\n",
       "      <td>transportation</td>\n",
       "      <td>motor</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[driver error, vehicle maintenance, sensor cal...</td>\n",
       "      <td>14</td>\n",
       "      <td>804</td>\n",
       "      <td>128</td>\n",
       "      <td>A motor fleet collision event occurred on a du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index  category                                        entity_name  \\\n",
       "0          0       esg  global manufacturing, logistics, and energy co...   \n",
       "1          1       esg                                   The organisation   \n",
       "2          2       esg  Synthetic ESG Report – Supply Chain and Climat...   \n",
       "3          3  incident                                 unspecified_entity   \n",
       "4          4  incident                        Motor Fleet Collision Event   \n",
       "\n",
       "   region                                        sector risk_type  \\\n",
       "0  global                                    industrial       esg   \n",
       "1  global  energy, logistics, heavy industry, transport       esg   \n",
       "2  global                                        energy       esg   \n",
       "3  global                                        marine  property   \n",
       "4  global                                transportation     motor   \n",
       "\n",
       "    time_horizon                              key_risk_factors_list  \\\n",
       "0  not_specified  [inconsistent ESG strategy, different interpre...   \n",
       "1    medium_term  [inconsistency in language used in internal do...   \n",
       "2  not_specified  [inconsistent expectations in environmental cr...   \n",
       "3  not_specified  [engine failure, grounding, irregular vibratio...   \n",
       "4  not_specified  [driver error, vehicle maintenance, sensor cal...   \n",
       "\n",
       "   num_risk_factors  summary_length_chars  summary_length_words  \\\n",
       "0                28                   804                   115   \n",
       "1                20                   804                   108   \n",
       "2                19                   804                   106   \n",
       "3                21                   804                   117   \n",
       "4                14                   804                   128   \n",
       "\n",
       "                                        risk_summary  \n",
       "0  The company's ESG strategy is still developing...  \n",
       "1  The organisation's energy transition strategy ...  \n",
       "2  The company's supply chain faces risks related...  \n",
       "3  The incident involved a synthetic marine vesse...  \n",
       "4  A motor fleet collision event occurred on a du...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Light normalisation and final cleaning\n",
    "\n",
    "# 1. Strip leading/trailing whitespace from all string-like columns\n",
    "str_cols = docs_df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# Exclude the list column, which is stored as object dtype but contains Python lists\n",
    "str_cols = [col for col in str_cols if col != \"key_risk_factors_list\"]\n",
    "\n",
    "for col in str_cols:\n",
    "    docs_df[col] = docs_df[col].str.strip()\n",
    "\n",
    "# 2. Enforce consistent lowercase for simple controlled vocab fields\n",
    "for col in [\"region\", \"risk_type\", \"time_horizon\", \"category\"]:\n",
    "    docs_df[col] = docs_df[col].str.lower()\n",
    "\n",
    "# 3. Fix missing entity_name\n",
    "docs_df[\"entity_name\"] = docs_df[\"entity_name\"].fillna(\"unspecified_entity\")\n",
    "\n",
    "# 4. Decide which columns to keep for modelling\n",
    "columns_to_keep = [\n",
    "    \"doc_index\",\n",
    "    \"category\",\n",
    "    \"entity_name\",\n",
    "    \"region\",\n",
    "    \"sector\",\n",
    "    \"risk_type\",\n",
    "    \"time_horizon\",\n",
    "    \"key_risk_factors_list\",\n",
    "    \"num_risk_factors\",\n",
    "    \"summary_length_chars\",\n",
    "    \"summary_length_words\",\n",
    "    \"risk_summary\",\n",
    "]\n",
    "\n",
    "docs_clean = docs_df[columns_to_keep].copy()\n",
    "\n",
    "# 5. Save the cleaned dataset\n",
    "CLEAN_PATH = DATA_PROCESSED_DIR / \"docs_clean.csv\"\n",
    "docs_clean.to_csv(CLEAN_PATH, index=False)\n",
    "\n",
    "print(\"Saved cleaned dataset to:\", CLEAN_PATH)\n",
    "print(\"\\nShape of docs_clean:\", docs_clean.shape)\n",
    "print(\"\\nColumns in docs_clean:\\n\", docs_clean.columns.tolist())\n",
    "print(\"\\nPreview of docs_clean:\\n\")\n",
    "docs_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ece54",
   "metadata": {},
   "source": [
    "## Summary of Notebook 02: EDA and Normalisation\n",
    "\n",
    "In this notebook we explored the extracted document level dataset produced in Notebook 01 and prepared it for modelling in the next stage.\n",
    "\n",
    "Key steps completed:\n",
    "\n",
    "1. **Loaded the extracted dataset**  \n",
    "   Imported `docs_extracted.csv` and inspected overall shape, schema, and field types.\n",
    "\n",
    "2. **Performed high level EDA**  \n",
    "   - Checked distributions of categories, regions, sectors, and risk types.  \n",
    "   - Investigated missing values and confirmed that only `entity_name` required imputation.  \n",
    "   - Examined variation in summary lengths and risk factor counts.\n",
    "\n",
    "3. **Converted raw fields into clean formats**  \n",
    "   - Used a robust regex based method to convert `key_risk_factors` into real Python lists.  \n",
    "   - Created early derived features including  \n",
    "     `num_risk_factors`, `summary_length_chars`, and `summary_length_words`.\n",
    "\n",
    "4. **Explored relationships in the derived features**  \n",
    "   - Compared averages by `category` and `risk_type`.  \n",
    "   - Observed clear and meaningful variation that will be useful for modelling.\n",
    "\n",
    "5. **Light normalisation and final cleaning**  \n",
    "   - Stripped whitespace from text fields.  \n",
    "   - Lowercased consistent categorical fields.  \n",
    "   - Filled missing `entity_name` values with a clean placeholder.  \n",
    "   - Selected and saved a tidy set of modelling ready columns.\n",
    "\n",
    "6. **Saved the cleaned dataset**  \n",
    "   Exported the final dataset to  \n",
    "   `data/processed/docs_clean.csv`, which will be the input to Notebook 03.\n",
    "\n",
    "The dataset is now clean, consistent, and ready for feature engineering and model building.  \n",
    "The next notebook will focus on constructing features, handling categorical variables, building baseline models, and interpreting the results in a risk scoring context.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI Insurance Project",
   "language": "python",
   "name": "genai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
