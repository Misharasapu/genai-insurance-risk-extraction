{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e68fc4",
   "metadata": {},
   "source": [
    "# Notebook 01: GenAI Extraction and Document Chunking\n",
    "\n",
    "This notebook starts the first stage of the end to end GenAI insurance document pipeline. The goal is to take a collection of long, unstructured documents and convert them into structured JSON outputs that can be analysed in later notebooks.\n",
    "\n",
    "The raw dataset includes three document types stored in the data/raw folder:  \n",
    "• Insurance policy wording style documents  \n",
    "• ESG style sustainability reports  \n",
    "• Incident and loss investigation summaries  \n",
    "\n",
    "Each document is synthetic, created through paraphrasing or original generation so that it mimics the style, tone, and complexity of real insurance documents without raising copyright issues. The documents are intentionally messy, unstructured, and varied in length to reflect the challenges seen in real underwriting and claims workflows.\n",
    "\n",
    "Insurance related documents are often long and difficult to process. Important details such as entities, regions, risk factors, exclusions, operational failures, and ESG concerns are spread across multiple paragraphs. Manual extraction is slow and inconsistent. This notebook focuses on building a reproducible GenAI workflow that can extract structured fields in a reliable way.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Load the raw documents from data/raw  \n",
    "2. Split each document into smaller chunks for safer extraction  \n",
    "3. Define a consistent JSON schema for the key fields we want to extract  \n",
    "4. Build a clear prompt template that enforces structure and avoids hallucination  \n",
    "5. Run the extraction process using helper functions in src  \n",
    "6. Apply a simple MapReduce style pattern by merging chunk outputs  \n",
    "7. Validate and save the final JSON outputs into data/processed  \n",
    "\n",
    "The goal is to produce clean, validated, and consistent structured data. These outputs will then be used in Notebook 02 for EDA and in Notebook 03 for feature engineering and classification. This mirrors practical workflows in insurtech companies, where large volumes of unstructured documents must be prepared and structured before modelling or decision support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07aa14",
   "metadata": {},
   "source": [
    "## Step 1: Set up imports and locate the raw documents\n",
    "\n",
    "Before we can do any chunking or GenAI extraction, we need a reliable way to point the notebook at the `data/raw` folder and see exactly which files we are working with.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Import a small set of core Python utilities  \n",
    "- Define a `PROJECT_ROOT` and `DATA_RAW_DIR` using `pathlib.Path` so paths are robust  \n",
    "- List all text files in the three raw subfolders: `policies`, `esg`, and `incidents`  \n",
    "- Store these file paths in a simple dictionary for later steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\n",
      "Raw data directory: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\raw\n",
      "\n",
      "Discovered raw documents:\n",
      "- esg: 3 files\n",
      "  • esg_corporate_sustainability.txt\n",
      "  • esg_energy_transition.txt\n",
      "  • esg_supply_chain_governance.txt\n",
      "- incident: 3 files\n",
      "  • incident_marine_grounding.txt\n",
      "  • incident_motor_fleet_collision.txt\n",
      "  • incident_property_fire.txt\n",
      "- policy: 8 files\n",
      "  • auto_insurance_policy_synthetic.txt\n",
      "  • businessowners_insurance_synthetic_01.txt\n",
      "  • cyber_insurance_policy_synthetic.txt\n",
      "  • group_life_policy_practice_unstructured_v1.txt\n",
      "  • homeowners_declarations_synthetic.txt\n",
      "  • homeowners_policy_ho3_synthetic.txt\n",
      "  • travel_insurance_policy_synthetic_allianz.txt\n",
      "  • travel_insurance_policy_synthetic_CHI.txt\n",
      "\n",
      "Total number of documents: 14\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1) Define project root and raw data directory\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {DATA_RAW_DIR}\")\n",
    "\n",
    "# 2) Collect documents by category (policies, esg, incidents)\n",
    "documents_by_category = {}\n",
    "\n",
    "for category_dir in sorted(DATA_RAW_DIR.iterdir()):\n",
    "    if category_dir.is_dir():\n",
    "        txt_files = sorted(category_dir.glob(\"*.txt\"))\n",
    "        documents_by_category[category_dir.name] = txt_files\n",
    "\n",
    "# 3) Print a short summary of what we found\n",
    "print(\"\\nDiscovered raw documents:\")\n",
    "for category, paths in documents_by_category.items():\n",
    "    print(f\"- {category}: {len(paths)} files\")\n",
    "    for path in paths:\n",
    "        print(f\"  • {path.name}\")\n",
    "\n",
    "# 4) Optional: a flat list of all documents (for later steps if needed)\n",
    "all_documents = [p for paths in documents_by_category.values() for p in paths]\n",
    "print(f\"\\nTotal number of documents: {len(all_documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994a031",
   "metadata": {},
   "source": [
    "## Step 2: Load document contents and create a simple overview\n",
    "\n",
    "Now that we know where the files are and how many we have in each category, the next step is to actually **load the text contents** into memory and build a simple summary table.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Read each `.txt` file into a Python string  \n",
    "- Capture basic metadata for each document  \n",
    "  - category (policies, esg, incidents)  \n",
    "  - filename  \n",
    "  - full filesystem path  \n",
    "  - character count  \n",
    "  - word count  \n",
    "  - a short text preview  \n",
    "- Store everything in a pandas `DataFrame` so we can inspect the documents in a structured way\n",
    "\n",
    "\n",
    "This overview will help us:\n",
    "\n",
    "- Decide sensible chunk sizes later  \n",
    "- Check that the documents have realistic lengths  \n",
    "- Quickly spot any weird or empty files before we start chunking and extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ab7f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>714</td>\n",
       "      <td>4958</td>\n",
       "      <td>Synthetic ESG Report – Corporate Sustainabilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>847</td>\n",
       "      <td>5874</td>\n",
       "      <td>Synthetic ESG Report – Energy Transition and E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>823</td>\n",
       "      <td>5755</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>778</td>\n",
       "      <td>4941</td>\n",
       "      <td>Synthetic Marine Incident Report – Engine Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_motor_fleet_collision.txt</td>\n",
       "      <td>788</td>\n",
       "      <td>5051</td>\n",
       "      <td>Synthetic Incident Report – Motor Fleet Collis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_property_fire.txt</td>\n",
       "      <td>789</td>\n",
       "      <td>5086</td>\n",
       "      <td>Synthetic Incident Report – Commercial Propert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>1025</td>\n",
       "      <td>6981</td>\n",
       "      <td>Synthetic Auto Insurance Practice Document  (f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>policy</td>\n",
       "      <td>businessowners_insurance_synthetic_01.txt</td>\n",
       "      <td>1896</td>\n",
       "      <td>12083</td>\n",
       "      <td>Synthetic Businessowners Insurance Practice Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>policy</td>\n",
       "      <td>cyber_insurance_policy_synthetic.txt</td>\n",
       "      <td>1089</td>\n",
       "      <td>7425</td>\n",
       "      <td>Synthetic Cyber Insurance Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>policy</td>\n",
       "      <td>group_life_policy_practice_unstructured_v1.txt</td>\n",
       "      <td>1376</td>\n",
       "      <td>8881</td>\n",
       "      <td>Synthetic Group Life Insurance Practice Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_declarations_synthetic.txt</td>\n",
       "      <td>1055</td>\n",
       "      <td>7149</td>\n",
       "      <td>Synthetic Homeowners Declarations Page Practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_policy_ho3_synthetic.txt</td>\n",
       "      <td>907</td>\n",
       "      <td>6145</td>\n",
       "      <td>Synthetic Homeowners HO-3 Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_allianz.txt</td>\n",
       "      <td>945</td>\n",
       "      <td>6357</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_CHI.txt</td>\n",
       "      <td>859</td>\n",
       "      <td>5838</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                        filename  n_words  \\\n",
       "0        esg                esg_corporate_sustainability.txt      714   \n",
       "1        esg                       esg_energy_transition.txt      847   \n",
       "2        esg                 esg_supply_chain_governance.txt      823   \n",
       "3   incident                   incident_marine_grounding.txt      778   \n",
       "4   incident              incident_motor_fleet_collision.txt      788   \n",
       "5   incident                      incident_property_fire.txt      789   \n",
       "6     policy             auto_insurance_policy_synthetic.txt     1025   \n",
       "7     policy       businessowners_insurance_synthetic_01.txt     1896   \n",
       "8     policy            cyber_insurance_policy_synthetic.txt     1089   \n",
       "9     policy  group_life_policy_practice_unstructured_v1.txt     1376   \n",
       "10    policy           homeowners_declarations_synthetic.txt     1055   \n",
       "11    policy             homeowners_policy_ho3_synthetic.txt      907   \n",
       "12    policy   travel_insurance_policy_synthetic_allianz.txt      945   \n",
       "13    policy       travel_insurance_policy_synthetic_CHI.txt      859   \n",
       "\n",
       "    n_chars                                            preview  \n",
       "0      4958  Synthetic ESG Report – Corporate Sustainabilit...  \n",
       "1      5874  Synthetic ESG Report – Energy Transition and E...  \n",
       "2      5755  Synthetic ESG Report – Supply Chain and Climat...  \n",
       "3      4941  Synthetic Marine Incident Report – Engine Fail...  \n",
       "4      5051  Synthetic Incident Report – Motor Fleet Collis...  \n",
       "5      5086  Synthetic Incident Report – Commercial Propert...  \n",
       "6      6981  Synthetic Auto Insurance Practice Document  (f...  \n",
       "7     12083  Synthetic Businessowners Insurance Practice Do...  \n",
       "8      7425  Synthetic Cyber Insurance Practice Document  (...  \n",
       "9      8881  Synthetic Group Life Insurance Practice Docume...  \n",
       "10     7149  Synthetic Homeowners Declarations Page Practic...  \n",
       "11     6145  Synthetic Homeowners HO-3 Practice Document  (...  \n",
       "12     6357  Synthetic Travel Insurance Practice Document  ...  \n",
       "13     5838  Synthetic Travel Insurance Practice Document (...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load contents of each document into memory and collect basic stats\n",
    "records = []\n",
    "\n",
    "for category, paths in documents_by_category.items():\n",
    "    for path in paths:\n",
    "        # Read the full text of the file\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        # Build a short, single line preview for quick inspection\n",
    "        preview = text[:400].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        # Append a record (one row) for this document\n",
    "        records.append(\n",
    "            {\n",
    "                \"category\": category,\n",
    "                \"filename\": path.name,\n",
    "                \"path\": path,\n",
    "                \"n_chars\": len(text),\n",
    "                \"n_words\": len(text.split()),\n",
    "                \"preview\": preview,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# 2) Create a DataFrame with one row per document\n",
    "docs_df = pd.DataFrame(records)\n",
    "\n",
    "# 3) Show a compact summary\n",
    "print(\"Document overview:\")\n",
    "display(\n",
    "    docs_df[[\"category\", \"filename\", \"n_words\", \"n_chars\", \"preview\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef658bb",
   "metadata": {},
   "source": [
    "## Step 3: Apply a consistent text chunking strategy\n",
    "\n",
    "Now that the helper function has been implemented in `src/chunking.py`, we can import it and use it to split each document into manageable chunks for LLM processing.\n",
    "\n",
    "In this project we use a word based chunking strategy with the following design choices:\n",
    "\n",
    "- Split text into chunks of approximately 250 words.\n",
    "- Use an overlap of 50 words so that important details near chunk boundaries are preserved.\n",
    "- Keep the parameters flexible so that chunk sizes can be adjusted later without changing the pipeline.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Import the `chunk_text` helper function from `src/chunking.py`.\n",
    "2. Apply it to a single example document.\n",
    "3. Inspect the number of chunks and preview the first few to confirm that the behaviour is sensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24f9fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example document: esg_corporate_sustainability.txt\n",
      "Total words in document: 714\n",
      "Number of chunks created: 4\n",
      "\n",
      "--- Chunk 0 (first 40 words) ---\n",
      "Synthetic ESG Report – Corporate Sustainability Narrative (fully synthetic paraphrased text created for training and GenAI extraction testing; not based on any copyrighted ESG document) (inspired by: corporate ESG and sustainability disclosures from global manufacturing, logistics, and energy companies) The\n",
      "\n",
      "--- Chunk 1 (first 40 words) ---\n",
      "the footnotes said the reductions were influenced by lower production volumes rather than actual efficiency improvements. No single team seemed responsible for consolidating the data, which led to confusion over which version was the most accurate. The company recycled some\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure src/ is on the Python path\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import the helper function\n",
    "from src.chunking import chunk_text\n",
    "\n",
    "# Test on one example document\n",
    "example_row = docs_df.iloc[0]\n",
    "example_text = example_row[\"path\"].read_text(encoding=\"utf-8\")\n",
    "\n",
    "example_chunks = chunk_text(example_text, max_words=250, overlap=50)\n",
    "\n",
    "print(f\"Example document: {example_row['filename']}\")\n",
    "print(f\"Total words in document: {example_row['n_words']}\")\n",
    "print(f\"Number of chunks created: {len(example_chunks)}\\n\")\n",
    "\n",
    "# Preview the first two chunks\n",
    "for i, chunk in enumerate(example_chunks[:2]):\n",
    "    print(f\"--- Chunk {i} (first 40 words) ---\")\n",
    "    print(\" \".join(chunk.split()[:40]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a303c49",
   "metadata": {},
   "source": [
    "## Step 4: Chunk all documents and build a chunk level table\n",
    "\n",
    "In the previous step we confirmed that the `chunk_text` helper function produces sensible chunks for a single document. The next step is to apply this function to every document in the corpus and create a structured table of chunks.\n",
    "\n",
    "The goal of this step is to move from a **document level view** (`docs_df`) to a **chunk level view** that is suitable for LLM extraction.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Loop over all rows in `docs_df` and apply `chunk_text` to each document.\n",
    "2. For each chunk, record the following metadata:\n",
    "   - `category` (policies, esg, incidents)\n",
    "   - `filename`\n",
    "   - `doc_index` (index of the document in `docs_df`)\n",
    "   - `chunk_index` (position of the chunk within that document)\n",
    "   - `chunk_text`\n",
    "   - `n_words_chunk` (word count in the chunk)\n",
    "3. Store all chunk records in a new pandas DataFrame called `chunks_df`.\n",
    "\n",
    "The `chunks_df` table will have one row per chunk and will serve as the main input for the LLM extraction step. This makes it easy to track where each chunk came from and to aggregate results back to the document level later in the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd3b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk level overview:\n",
      "- Number of documents: 14\n",
      "- Total number of chunks: 76\n",
      "- Average chunks per document: 5.43\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>n_words_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index category                          filename  chunk_index  \\\n",
       "0          0      esg  esg_corporate_sustainability.txt            0   \n",
       "1          0      esg  esg_corporate_sustainability.txt            1   \n",
       "2          0      esg  esg_corporate_sustainability.txt            2   \n",
       "3          0      esg  esg_corporate_sustainability.txt            3   \n",
       "4          1      esg         esg_energy_transition.txt            0   \n",
       "5          1      esg         esg_energy_transition.txt            1   \n",
       "6          1      esg         esg_energy_transition.txt            2   \n",
       "7          1      esg         esg_energy_transition.txt            3   \n",
       "8          1      esg         esg_energy_transition.txt            4   \n",
       "9          2      esg   esg_supply_chain_governance.txt            0   \n",
       "\n",
       "   n_words_chunk  \n",
       "0            250  \n",
       "1            250  \n",
       "2            250  \n",
       "3            114  \n",
       "4            250  \n",
       "5            250  \n",
       "6            250  \n",
       "7            247  \n",
       "8             47  \n",
       "9            250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Chunk all documents and build a chunk-level DataFrame\n",
    "\n",
    "chunk_records = []\n",
    "\n",
    "for doc_index, row in docs_df.iterrows():\n",
    "    # Read full text for this document\n",
    "    text = row[\"path\"].read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Generate chunks using the helper\n",
    "    chunks = chunk_text(text, max_words=250, overlap=50)\n",
    "\n",
    "    # Create one record per chunk\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        chunk_records.append(\n",
    "            {\n",
    "                \"doc_index\": doc_index,\n",
    "                \"category\": row[\"category\"],\n",
    "                \"filename\": row[\"filename\"],\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"chunk_text\": chunk,\n",
    "                \"n_words_chunk\": len(chunk.split()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Build the chunk-level DataFrame\n",
    "chunks_df = pd.DataFrame(chunk_records)\n",
    "\n",
    "print(\"Chunk level overview:\")\n",
    "print(f\"- Number of documents: {len(docs_df)}\")\n",
    "print(f\"- Total number of chunks: {len(chunks_df)}\")\n",
    "print(f\"- Average chunks per document: {len(chunks_df) / len(docs_df):.2f}\\n\")\n",
    "\n",
    "display(\n",
    "    chunks_df[[\"doc_index\", \"category\", \"filename\", \"chunk_index\", \"n_words_chunk\"]]\n",
    "    .head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89952f2d",
   "metadata": {},
   "source": [
    "## Step 5: Define the JSON schema and controlled vocabularies\n",
    "\n",
    "Before calling the LLM, we need a clear and consistent definition of the structured output we expect from each chunk. This ensures that:\n",
    "\n",
    "- The model always returns the same fields.\n",
    "- Values are constrained where appropriate.\n",
    "- Validation in later steps is straightforward.\n",
    "\n",
    "In this project, each extraction call is expected to return the following JSON structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"entity_name\": \"\",\n",
    "  \"region\": \"\",\n",
    "  \"sector\": \"\",\n",
    "  \"risk_type\": \"\",\n",
    "  \"time_horizon\": \"\",\n",
    "  \"key_risk_factors\": [],\n",
    "  \"risk_summary\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618e187f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Field Schema:\n",
      "- entity_name: string\n",
      "- region: string\n",
      "- sector: string\n",
      "- risk_type: string\n",
      "- time_horizon: string\n",
      "- key_risk_factors: list\n",
      "- risk_summary: string\n",
      "\n",
      "Allowed Risk Types:\n",
      "['property', 'marine', 'motor', 'cyber', 'liability', 'health', 'travel', 'esg', 'operational', 'other']\n",
      "\n",
      "Allowed Regions:\n",
      "['global', 'europe', 'north_america', 'asia_pacific', 'latin_america', 'middle_east_africa']\n",
      "\n",
      "Allowed Time Horizons:\n",
      "['short_term', 'medium_term', 'long_term', 'multi_horizon', 'not_specified']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure src/ is on the Python path\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import schema and vocabularies\n",
    "from src.validation import (\n",
    "    FIELD_SCHEMA,\n",
    "    ALLOWED_RISK_TYPES,\n",
    "    ALLOWED_REGIONS,\n",
    "    ALLOWED_TIME_HORIZONS,\n",
    ")\n",
    "\n",
    "print(\"JSON Field Schema:\")\n",
    "for field, dtype in FIELD_SCHEMA.items():\n",
    "    print(f\"- {field}: {dtype}\")\n",
    "\n",
    "print(\"\\nAllowed Risk Types:\")\n",
    "print(ALLOWED_RISK_TYPES)\n",
    "\n",
    "print(\"\\nAllowed Regions:\")\n",
    "print(ALLOWED_REGIONS)\n",
    "\n",
    "print(\"\\nAllowed Time Horizons:\")\n",
    "print(ALLOWED_TIME_HORIZONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6826a0b",
   "metadata": {},
   "source": [
    "## Step 6: Load and test the LLM extraction prompt template\n",
    "\n",
    "With the JSON schema and controlled vocabularies defined, the next step is to use a clear, repeatable prompt template for the LLM. The goal is to ensure that every call to LLaMA 3 (via the Groq API) returns a strict JSON object that matches the agreed schema and uses only the allowed values where specified.\n",
    "\n",
    "To keep the notebook focused on workflow rather than long prompt strings, the prompt template is defined inside `src/prompt_template.py`. That helper module:\n",
    "\n",
    "- Stores the base prompt text, including the JSON schema and constraints.\n",
    "- Inserts the controlled vocabularies for `risk_type`, `region`, and `time_horizon`.\n",
    "- Provides a `build_prompt()` function that takes a single chunk of text and returns a fully formatted prompt.\n",
    "- Enforces that the model should return valid JSON only, with no extra commentary or invented information.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Import the `build_prompt` helper from `src/prompt_template.py`.\n",
    "2. Apply it to one example chunk from `chunks_df`.\n",
    "3. Inspect the first part of the resulting prompt to confirm that:\n",
    "   - the schema is included,\n",
    "   - the allowed values are correctly inserted,\n",
    "   - the chunk content appears in the intended place.\n",
    "\n",
    "This prompt template will act as the standard interface between the chunk level inputs and the LLM extraction logic that we implement in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7c2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant that extracts structured risk information from insurance related text.\n",
      "\n",
      "Your task:\n",
      "- Read the provided document chunk carefully.\n",
      "- Extract the requested fields based only on information in this chunk.\n",
      "- If a field is not mentioned or cannot be inferred with high confidence, leave it as an empty string \"\" or an empty list [].\n",
      "\n",
      "Output format:\n",
      "Return a single JSON object with exactly these fields:\n",
      "- entity_name (string)\n",
      "- region (string, one of: global, europe, north_america, asia_pacific, latin_america, middle_east_africa)\n",
      "- sector (string)\n",
      "- risk_type (string, one of: property, marine, motor, cyber, liability, health, travel, esg, operational, other)\n",
      "- time_horizon (string, one of: short_term, medium_term, long_term, multi_horizon, not_specified)\n",
      "- key_risk_factors (\n"
     ]
    }
   ],
   "source": [
    "from src.prompt_template import build_prompt\n",
    "\n",
    "# Test prompt building on the first chunk\n",
    "test_chunk = chunks_df.loc[0, \"chunk_text\"]\n",
    "test_prompt = build_prompt(test_chunk)\n",
    "\n",
    "print(test_prompt[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f049512",
   "metadata": {},
   "source": [
    "## Step 7: Wire up the LLM extraction helper and test on a single chunk\n",
    "\n",
    "With the prompt template and JSON schema in place, the next step is to connect the notebook to the LLM extraction helper implemented in `src/extraction.py`. The aim is to send a single chunk of text to LLaMA 3 (via the Groq API), receive a structured JSON response, and validate it against the expected schema.\n",
    "\n",
    "The extraction helper is responsible for:\n",
    "\n",
    "1. Loading the Groq API key from the `.env` file using `python-dotenv`.\n",
    "2. Creating a Groq client that will send requests to the LLaMA 3 model.\n",
    "3. Building the final prompt for a given chunk using `build_prompt` from `src/prompt_template.py`.\n",
    "4. Calling the Groq chat completion API with:\n",
    "   - a fixed system message that describes the model’s role as a JSON extraction assistant\n",
    "   - a user message containing the formatted prompt and chunk text\n",
    "5. Parsing the model output as JSON.\n",
    "6. Validating the parsed JSON using `validate_extracted_json` from `src/validation.py`.\n",
    "7. Returning a cleaned Python dictionary that matches the agreed schema, or `None` if extraction fails after the allowed number of retries.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Import the `call_llm_on_chunk` helper from `src/extraction.py`.\n",
    "2. Select a single example chunk from `chunks_df`.\n",
    "3. Run the helper once on that chunk.\n",
    "4. Inspect the returned dictionary to confirm that:\n",
    "   - all expected fields are present,\n",
    "   - controlled fields (such as `risk_type` and `region`) use allowed values, and\n",
    "   - the summary fields (`key_risk_factors`, `risk_summary`) look reasonable for the input text.\n",
    "\n",
    "This step serves as a sanity check before running the extraction over all chunks in a later step. It confirms that the end to end flow from chunk text, through prompt building, API call, JSON parsing, and validation is working correctly on a small example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1405749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document index: 0\n",
      "Category: esg\n",
      "Filename: esg_corporate_sustainability.txt\n",
      "\n",
      "First 300 characters of the chunk:\n",
      "\n",
      "Synthetic ESG Report – Corporate Sustainability Narrative (fully synthetic paraphrased text created for training and GenAI extraction testing; not based on any copyrighted ESG document) (inspired by: corporate ESG and sustainability disclosures from global manufacturing, logistics, and energy compan\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Extraction result (Python dict):\n",
      "\n",
      "{'entity_name': 'global manufacturing, logistics, and energy companies', 'region': 'global', 'sector': 'manufacturing, logistics, and energy', 'risk_type': 'esg', 'time_horizon': 'not_specified', 'key_risk_factors': ['inconsistent ESG strategy', 'different interpretations of sustainability across regions', 'inconsistent documentation', 'difficulty in comparing environmental impacts', 'reliability issues with contractor-operated equipment', 'confusion over data accuracy'], 'risk_summary': \"The company's ESG strategy is still developing and not consistently implemented across business units, creating an inconsistent picture when trying to present a single global ESG narrative. Different regions operate with their own interpretations of what sustainability means, and the level of documentation varies. The company has made some attempts to quantify environmental impacts, but the numbers are difficult to compare due to changing boundaries and assumptions.\"}\n",
      "\n",
      "Keys in the extracted dictionary:\n",
      "['entity_name', 'region', 'sector', 'risk_type', 'time_horizon', 'key_risk_factors', 'risk_summary']\n"
     ]
    }
   ],
   "source": [
    "from src.extraction import call_llm_on_chunk\n",
    "\n",
    "# Select a single example chunk to test the full extraction pipeline\n",
    "example_row = chunks_df.iloc[0]\n",
    "example_chunk_text = example_row[\"chunk_text\"]\n",
    "\n",
    "print(\"Document index:\", example_row[\"doc_index\"])\n",
    "print(\"Category:\", example_row[\"category\"])\n",
    "print(\"Filename:\", example_row[\"filename\"])\n",
    "print(\"\\nFirst 300 characters of the chunk:\\n\")\n",
    "print(example_chunk_text[:300])\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Call the LLM extraction helper on this single chunk\n",
    "extracted_dict = call_llm_on_chunk(example_chunk_text, max_retries=1)\n",
    "\n",
    "print(\"Extraction result (Python dict):\\n\")\n",
    "print(extracted_dict)\n",
    "\n",
    "# If the result is not None, show the keys to confirm the schema\n",
    "if extracted_dict is not None:\n",
    "    print(\"\\nKeys in the extracted dictionary:\")\n",
    "    print(list(extracted_dict.keys()))\n",
    "else:\n",
    "    print(\"\\nExtraction failed or returned None.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14215d79",
   "metadata": {},
   "source": [
    "## Step 8: Test extraction on a small sample of chunks\n",
    "\n",
    "Before running the LLM across the entire corpus, it is important to test the full extraction pipeline on a small, diverse sample of chunks. This acts as a smoke test for both the prompt design and the `call_llm_on_chunk` helper.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Select a small subset of chunks from `chunks_df`, covering different document categories (policies, esg, incidents).\n",
    "2. Run `call_llm_on_chunk` on each selected chunk.\n",
    "3. Inspect the returned dictionaries to confirm that:\n",
    "   - All expected fields are present.\n",
    "   - `risk_type`, `region`, and `time_horizon` values come from the allowed vocabularies.\n",
    "   - `key_risk_factors` and `risk_summary` are reasonable given the input text.\n",
    "4. Log any failures or strange outputs so that we can adjust the prompt or validation logic before scaling up.\n",
    "\n",
    "This step gives confidence that the end to end extraction pipeline behaves sensibly across different document types, before we move on to the full Map step over all chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c9e7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for manual inspection: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>n_words_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index  category                             filename  chunk_index  \\\n",
       "0          0       esg     esg_corporate_sustainability.txt            0   \n",
       "1          0       esg     esg_corporate_sustainability.txt            1   \n",
       "2          3  incident        incident_marine_grounding.txt            0   \n",
       "3          3  incident        incident_marine_grounding.txt            1   \n",
       "4          6    policy  auto_insurance_policy_synthetic.txt            0   \n",
       "5          6    policy  auto_insurance_policy_synthetic.txt            1   \n",
       "\n",
       "   n_words_chunk  \n",
       "0            250  \n",
       "1            250  \n",
       "2            250  \n",
       "3            250  \n",
       "4            250  \n",
       "5            250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Sample 1 of 6\n",
      "Category      : esg\n",
      "Filename      : esg_corporate_sustainability.txt\n",
      "Document index: 0\n",
      "Chunk index   : 0\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "Synthetic ESG Report – Corporate Sustainability Narrative (fully synthetic paraphrased text created for training and GenAI extraction testing; not based on any copyrighted ESG document) (inspired by: corporate ESG and sustainability disclosures from \n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : global manufacturing, logistics, and energy companies\n",
      "  region       : global\n",
      "  risk_type    : esg\n",
      "  time_horizon : not_specified\n",
      "  risk_summary :\n",
      "  The company's ESG strategy is still developing and not consistently implemented across business units, creating an inconsistent picture when trying to present a single global ESG narrative. Different regions operate with their own interpretations of what sustainability means, and the level of docume\n",
      "\n",
      "================================================================================\n",
      "Sample 2 of 6\n",
      "Category      : esg\n",
      "Filename      : esg_corporate_sustainability.txt\n",
      "Document index: 0\n",
      "Chunk index   : 1\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "the footnotes said the reductions were influenced by lower production volumes rather than actual efficiency improvements. No single team seemed responsible for consolidating the data, which led to confusion over which version was the most accurate. T\n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : \n",
      "  region       : \n",
      "  risk_type    : operational\n",
      "  time_horizon : short_term\n",
      "  risk_summary :\n",
      "  The company's operational risk is high due to inconsistent data collection, vague details on recycling, and estimated waste reduction. The lack of formal policy on waste reduction and inconsistent reporting on social issues also contribute to the risk. Additionally, incomplete supporting documents f\n",
      "\n",
      "================================================================================\n",
      "Sample 3 of 6\n",
      "Category      : incident\n",
      "Filename      : incident_marine_grounding.txt\n",
      "Document index: 3\n",
      "Chunk index   : 0\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "Synthetic Marine Incident Report – Engine Failure and Grounding (fully synthetic narrative created for training and GenAI extraction testing; not based on any real casualty report) The incident occurred sometime during the early hours of a late autum\n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : \n",
      "  region       : global\n",
      "  risk_type    : property\n",
      "  time_horizon : not_specified\n",
      "  risk_summary :\n",
      "  The incident involved a synthetic marine vessel experiencing engine failure and grounding due to various factors, including irregular vibrations, inconsistent engine room logs, and a route deviation. The exact time and circumstances of the event are unclear due to conflicting accounts from crew memb\n",
      "\n",
      "================================================================================\n",
      "Sample 4 of 6\n",
      "Category      : incident\n",
      "Filename      : incident_marine_grounding.txt\n",
      "Document index: 3\n",
      "Chunk index   : 1\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "screenshot that was provided later shows a slight deviation to the north, possibly due to traffic avoidance or miscommunication between the bridge team and the lookout. There was also a brief mention of radar clutter caused by rain, though there is n\n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : \n",
      "  region       : middle_east_africa\n",
      "  risk_type    : operational\n",
      "  time_horizon : short_term\n",
      "  risk_summary :\n",
      "  The vessel experienced a sudden loss of propulsion due to a main engine shutdown, leading to drifting towards a rocky shoreline. Attempts to restart the engine and drop anchor were unsuccessful. The incident was complicated by conflicting information and intermittent communications.\n",
      "\n",
      "================================================================================\n",
      "Sample 5 of 6\n",
      "Category      : policy\n",
      "Filename      : auto_insurance_policy_synthetic.txt\n",
      "Document index: 6\n",
      "Chunk index   : 0\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "Synthetic Auto Insurance Practice Document (fully synthetic paraphrased text created for training and GenAI extraction testing; not based on any copyrighted policy wording) (inspired by: Allstate Personal Auto Insurance Policy, but not derived from a\n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : Synthetic Auto Insurance\n",
      "  region       : north_america\n",
      "  risk_type    : motor\n",
      "  time_horizon : short_term\n",
      "  risk_summary :\n",
      "  The motor vehicle insurance policy has risks associated with bodily injury, property damage, and liability, particularly in determining fault and understanding limits of liability. Policyholders may misunderstand the limits of liability, leading to increased premiums or reduced coverage. The insurer\n",
      "\n",
      "================================================================================\n",
      "Sample 6 of 6\n",
      "Category      : policy\n",
      "Filename      : auto_insurance_policy_synthetic.txt\n",
      "Document index: 6\n",
      "Chunk index   : 1\n",
      "Chunk length  : 250 words\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk preview (first 250 characters):\n",
      "\n",
      "separate for bodily injury per person, bodily injury per accident, and property damage, but misunderstandings occur when policyholders assume the higher of the three values applies across all losses. Some drivers increase these limits at renewal with\n",
      "\n",
      "Calling LLM extraction helper...\n",
      "\n",
      "Extraction result (summary):\n",
      "  entity_name  : \n",
      "  region       : \n",
      "  risk_type    : liability\n",
      "  time_horizon : not_specified\n",
      "  risk_summary :\n",
      "  Policyholders may misunderstand policy limits, leading to inconsistent coverage. Insurers may intervene in some cases, but interventions are not always consistent. Additionally, customers may confuse deductibles and experience disputes over zero deductible options, while glass claims and medical exp\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Test extraction on a small sample of chunks\n",
    "\n",
    "from src.extraction import call_llm_on_chunk\n",
    "\n",
    "# 1) Pick a small, diverse sample of chunks\n",
    "#    Here we take the first 2 chunks from each category, if available.\n",
    "sample_chunks = (\n",
    "    chunks_df\n",
    "    .groupby(\"category\", group_keys=False)\n",
    "    .head(2)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Sample size for manual inspection:\", len(sample_chunks))\n",
    "display(\n",
    "    sample_chunks[[\"doc_index\", \"category\", \"filename\", \"chunk_index\", \"n_words_chunk\"]]\n",
    ")\n",
    "\n",
    "# 2) Run the LLM extraction helper on each sampled chunk\n",
    "results = []\n",
    "\n",
    "for idx, row in sample_chunks.iterrows():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Sample {idx+1} of {len(sample_chunks)}\")\n",
    "    print(f\"Category      : {row['category']}\")\n",
    "    print(f\"Filename      : {row['filename']}\")\n",
    "    print(f\"Document index: {row['doc_index']}\")\n",
    "    print(f\"Chunk index   : {row['chunk_index']}\")\n",
    "    print(f\"Chunk length  : {row['n_words_chunk']} words\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Chunk preview (first 250 characters):\\n\")\n",
    "    print(row[\"chunk_text\"][:250])\n",
    "    print(\"\\nCalling LLM extraction helper...\\n\")\n",
    "\n",
    "    chunk_text = row[\"chunk_text\"]\n",
    "\n",
    "    extracted = call_llm_on_chunk(chunk_text, max_retries=2)\n",
    "    results.append(extracted)\n",
    "\n",
    "    print(\"Extraction result (summary):\")\n",
    "    if extracted is None:\n",
    "        print(\"  Extraction failed or returned None.\")\n",
    "    else:\n",
    "        # Safely get key fields with defaults\n",
    "        entity_name = extracted.get(\"entity_name\", \"\")\n",
    "        region = extracted.get(\"region\", \"\")\n",
    "        risk_type = extracted.get(\"risk_type\", \"\")\n",
    "        time_horizon = extracted.get(\"time_horizon\", \"\")\n",
    "        risk_summary = extracted.get(\"risk_summary\", \"\")\n",
    "\n",
    "        print(f\"  entity_name  : {entity_name}\")\n",
    "        print(f\"  region       : {region}\")\n",
    "        print(f\"  risk_type    : {risk_type}\")\n",
    "        print(f\"  time_horizon : {time_horizon}\")\n",
    "        print(\"  risk_summary :\")\n",
    "        print(\" \", risk_summary[:300])\n",
    "\n",
    "# Optional: keep results together for later inspection if needed\n",
    "sample_extractions = results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e21b31",
   "metadata": {},
   "source": [
    "## Step 9: Map phase across all chunks\n",
    "\n",
    "Now that the extraction helper has been tested on individual chunks and on a small, diverse sample, we can scale the process to the full corpus. The goal of this step is to apply the LLM extraction function to every chunk in `chunks_df`.\n",
    "\n",
    "Each row in `chunks_df` represents a small slice of a longer document. In the Map phase we send each slice through the LLM and collect the structured JSON outputs. This produces a new DataFrame called `chunk_outputs_df` that contains, for every chunk:\n",
    "\n",
    "- The document index and filename  \n",
    "- The document category (`policies`, `esg`, or `incidents`)  \n",
    "- The chunk index within that document  \n",
    "- The extracted JSON dictionary returned by the LLM  \n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Loop over all rows in `chunks_df`.  \n",
    "2. For each row, pass the `chunk_text` to `call_llm_on_chunk`.  \n",
    "3. Store the returned Python dictionary together with the document and chunk identifiers.  \n",
    "4. Record any failures as `None` so that they can be handled safely later.  \n",
    "5. Convert the collected results into a new DataFrame called `chunk_outputs_df`.  \n",
    "\n",
    "Some chunks will not contain clear risk information or the LLM may occasionally fail. By keeping these as `None`, the pipeline remains robust and transparent. Once this Map phase is complete, we will have one row per chunk with its corresponding structured output. \n",
    "\n",
    "In the next step, we will move to the Reduce phase, where chunk level outputs are merged into a single document level record for each original file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fedc292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Map phase over all chunks (total: 76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 76/76 [08:27<00:00,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map phase complete in 507.74 seconds\n",
      "\n",
      "Chunk extraction output overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>extracted_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>{'entity_name': 'global manufacturing, logisti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>{'entity_name': '', 'region': '', 'sector': 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>{'entity_name': '', 'region': '', 'sector': 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>{'entity_name': '', 'region': 'global', 'secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>{'entity_name': 'The organisation', 'region': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>{'entity_name': '', 'region': 'global', 'secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>{'entity_name': '', 'region': '', 'sector': 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>{'entity_name': '', 'region': 'global', 'secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>{'entity_name': '', 'region': '', 'sector': 'E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>{'entity_name': 'Synthetic ESG Report – Supply...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index category                          filename  chunk_index  \\\n",
       "0          0      esg  esg_corporate_sustainability.txt            0   \n",
       "1          0      esg  esg_corporate_sustainability.txt            1   \n",
       "2          0      esg  esg_corporate_sustainability.txt            2   \n",
       "3          0      esg  esg_corporate_sustainability.txt            3   \n",
       "4          1      esg         esg_energy_transition.txt            0   \n",
       "5          1      esg         esg_energy_transition.txt            1   \n",
       "6          1      esg         esg_energy_transition.txt            2   \n",
       "7          1      esg         esg_energy_transition.txt            3   \n",
       "8          1      esg         esg_energy_transition.txt            4   \n",
       "9          2      esg   esg_supply_chain_governance.txt            0   \n",
       "\n",
       "                                      extracted_json  \n",
       "0  {'entity_name': 'global manufacturing, logisti...  \n",
       "1  {'entity_name': '', 'region': '', 'sector': 'i...  \n",
       "2  {'entity_name': '', 'region': '', 'sector': 'i...  \n",
       "3  {'entity_name': '', 'region': 'global', 'secto...  \n",
       "4  {'entity_name': 'The organisation', 'region': ...  \n",
       "5  {'entity_name': '', 'region': 'global', 'secto...  \n",
       "6  {'entity_name': '', 'region': '', 'sector': 'e...  \n",
       "7  {'entity_name': '', 'region': 'global', 'secto...  \n",
       "8  {'entity_name': '', 'region': '', 'sector': 'E...  \n",
       "9  {'entity_name': 'Synthetic ESG Report – Supply...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 76\n",
      "Number of failed extractions: 0\n"
     ]
    }
   ],
   "source": [
    "## Step 9: Map phase over all chunks (using tqdm progress bar)\n",
    "\n",
    "from src.extraction import call_llm_on_chunk\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "map_records = []\n",
    "\n",
    "total_chunks = len(chunks_df)\n",
    "print(f\"Starting Map phase over all chunks (total: {total_chunks})\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Wrap the iterator with tqdm for a progress bar\n",
    "for idx, row in tqdm(chunks_df.iterrows(), total=total_chunks):\n",
    "    chunk_text = row[\"chunk_text\"]\n",
    "\n",
    "    extracted = call_llm_on_chunk(chunk_text, max_retries=2)\n",
    "\n",
    "    map_records.append(\n",
    "        {\n",
    "            \"doc_index\": row[\"doc_index\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"filename\": row[\"filename\"],\n",
    "            \"chunk_index\": row[\"chunk_index\"],\n",
    "            \"extracted_json\": extracted,\n",
    "        }\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Map phase complete in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Build the chunk output DataFrame\n",
    "chunk_outputs_df = pd.DataFrame(map_records)\n",
    "\n",
    "print(\"\\nChunk extraction output overview:\")\n",
    "display(\n",
    "    chunk_outputs_df[\n",
    "        [\"doc_index\", \"category\", \"filename\", \"chunk_index\", \"extracted_json\"]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# Failure count\n",
    "n_failed = chunk_outputs_df[\"extracted_json\"].isna().sum()\n",
    "print(f\"\\nTotal chunks: {total_chunks}\")\n",
    "print(f\"Number of failed extractions: {n_failed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d6dd5",
   "metadata": {},
   "source": [
    "## Step 10: Reduce phase from chunk level to document level\n",
    "\n",
    "After completing the Map phase, we now have `chunk_outputs_df`, which contains one row per chunk and a column called `extracted_json` that stores the structured output from the LLM. The goal of this step is to merge all chunk level outputs that belong to the same document into a single consolidated record.\n",
    "\n",
    "This is the Reduce phase in the MapReduce pattern. Instead of treating each chunk independently, we aggregate the information from all chunks of a document to produce one clean document level summary.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Group the chunk outputs by `doc_index` so that all chunks from the same document are processed together.  \n",
    "2. For each document, merge the `extracted_json` dictionaries using simple rules:  \n",
    "   - For fields such as `entity_name`, `region`, `sector`, `risk_type`, and `time_horizon`, take the first non empty value or the most common value across chunks.  \n",
    "   - For `key_risk_factors`, combine lists from all chunks and remove duplicates.  \n",
    "   - For `risk_summary`, join short summaries from multiple chunks into one concise paragraph.  \n",
    "3. Construct a new DataFrame called `docs_extracted_df` that contains one row per original document, along with its merged fields.  \n",
    "4. Inspect a few rows to confirm that the merging strategy gives sensible results.  \n",
    "5. Save the merged document level outputs into `data/processed` for use in Notebook 02 (EDA) and Notebook 03 (feature engineering and classification).\n",
    "\n",
    "By the end of this step we will have transformed the noisy chunk level data into clean document level signals. This provides a reliable foundation for downstream analysis, mirroring how real insurtech pipelines summarise multiple paragraph level extractions into a single risk oriented view of each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a14a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document level extraction overview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\misha\\AppData\\Local\\Temp\\ipykernel_5628\\2370556573.py:39: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(reduce_document_group)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>n_chunks</th>\n",
       "      <th>n_chunks_with_data</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>region</th>\n",
       "      <th>sector</th>\n",
       "      <th>risk_type</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>key_risk_factors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>global manufacturing, logistics, and energy co...</td>\n",
       "      <td>global</td>\n",
       "      <td>industrial</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[inconsistent ESG strategy, different interpre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The organisation</td>\n",
       "      <td>global</td>\n",
       "      <td>energy, logistics, heavy industry, transport</td>\n",
       "      <td>esg</td>\n",
       "      <td>medium_term</td>\n",
       "      <td>[inconsistency in language used in internal do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "      <td>global</td>\n",
       "      <td>energy</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[inconsistent expectations in environmental cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>global</td>\n",
       "      <td>marine</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[engine failure, grounding, irregular vibratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_motor_fleet_collision.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Motor Fleet Collision Event</td>\n",
       "      <td>global</td>\n",
       "      <td>transportation</td>\n",
       "      <td>motor</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[driver error, vehicle maintenance, sensor cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>incident</td>\n",
       "      <td>incident_property_fire.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>north_america</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[faulty fire alarm panel, unusual smell, contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Synthetic Auto Insurance</td>\n",
       "      <td>north_america</td>\n",
       "      <td>insurance</td>\n",
       "      <td>liability</td>\n",
       "      <td>short_term</td>\n",
       "      <td>[bodily injury, property damage, liability, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>policy</td>\n",
       "      <td>businessowners_insurance_synthetic_01.txt</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>north_america</td>\n",
       "      <td>insurance</td>\n",
       "      <td>liability</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[exclusions, expected losses, expected or inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>policy</td>\n",
       "      <td>cyber_insurance_policy_synthetic.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Cyber Insurance</td>\n",
       "      <td>global</td>\n",
       "      <td>insurance</td>\n",
       "      <td>cyber</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[malicious attacks, accidental system failures...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>policy</td>\n",
       "      <td>group_life_policy_practice_unstructured_v1.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Synthetic Group Life Insurance</td>\n",
       "      <td>global</td>\n",
       "      <td>insurance</td>\n",
       "      <td>liability</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[discrepancies in reported earnings, late enro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_declarations_synthetic.txt</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>global</td>\n",
       "      <td>insurance</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[miscalculations of replacement cost estimates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_policy_ho3_synthetic.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Synthetic Homeowners HO-3</td>\n",
       "      <td>global</td>\n",
       "      <td>insurance</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[damage from a broad range of causes, exclusio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_allianz.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Allianz Travel Insurance Basic Plan</td>\n",
       "      <td>global</td>\n",
       "      <td>travel</td>\n",
       "      <td>travel</td>\n",
       "      <td>short_term</td>\n",
       "      <td>[medical emergencies, trip cancellations, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_CHI.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Synthetic Travel Insurance</td>\n",
       "      <td>global</td>\n",
       "      <td>travel</td>\n",
       "      <td>travel</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>[accidental medical emergencies, communication...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_index  category                                        filename  \\\n",
       "0           0       esg                esg_corporate_sustainability.txt   \n",
       "1           1       esg                       esg_energy_transition.txt   \n",
       "2           2       esg                 esg_supply_chain_governance.txt   \n",
       "3           3  incident                   incident_marine_grounding.txt   \n",
       "4           4  incident              incident_motor_fleet_collision.txt   \n",
       "5           5  incident                      incident_property_fire.txt   \n",
       "6           6    policy             auto_insurance_policy_synthetic.txt   \n",
       "7           7    policy       businessowners_insurance_synthetic_01.txt   \n",
       "8           8    policy            cyber_insurance_policy_synthetic.txt   \n",
       "9           9    policy  group_life_policy_practice_unstructured_v1.txt   \n",
       "10         10    policy           homeowners_declarations_synthetic.txt   \n",
       "11         11    policy             homeowners_policy_ho3_synthetic.txt   \n",
       "12         12    policy   travel_insurance_policy_synthetic_allianz.txt   \n",
       "13         13    policy       travel_insurance_policy_synthetic_CHI.txt   \n",
       "\n",
       "    n_chunks  n_chunks_with_data  \\\n",
       "0          4                   4   \n",
       "1          5                   5   \n",
       "2          5                   5   \n",
       "3          4                   4   \n",
       "4          4                   4   \n",
       "5          4                   4   \n",
       "6          6                   6   \n",
       "7         10                  10   \n",
       "8          6                   6   \n",
       "9          7                   7   \n",
       "10         6                   6   \n",
       "11         5                   5   \n",
       "12         5                   5   \n",
       "13         5                   5   \n",
       "\n",
       "                                          entity_name         region  \\\n",
       "0   global manufacturing, logistics, and energy co...         global   \n",
       "1                                    The organisation         global   \n",
       "2   Synthetic ESG Report – Supply Chain and Climat...         global   \n",
       "3                                                             global   \n",
       "4                         Motor Fleet Collision Event         global   \n",
       "5                                                      north_america   \n",
       "6                            Synthetic Auto Insurance  north_america   \n",
       "7                                                      north_america   \n",
       "8                                     Cyber Insurance         global   \n",
       "9                      Synthetic Group Life Insurance         global   \n",
       "10                                                            global   \n",
       "11                          Synthetic Homeowners HO-3         global   \n",
       "12                Allianz Travel Insurance Basic Plan         global   \n",
       "13                         Synthetic Travel Insurance         global   \n",
       "\n",
       "                                          sector  risk_type   time_horizon  \\\n",
       "0                                     industrial        esg  not_specified   \n",
       "1   energy, logistics, heavy industry, transport        esg    medium_term   \n",
       "2                                         energy        esg  not_specified   \n",
       "3                                         marine   property  not_specified   \n",
       "4                                 transportation      motor  not_specified   \n",
       "5                                  manufacturing   property  not_specified   \n",
       "6                                      insurance  liability     short_term   \n",
       "7                                      insurance  liability  not_specified   \n",
       "8                                      insurance      cyber  not_specified   \n",
       "9                                      insurance  liability  not_specified   \n",
       "10                                     insurance   property  not_specified   \n",
       "11                                     insurance   property  not_specified   \n",
       "12                                        travel     travel     short_term   \n",
       "13                                        travel     travel  not_specified   \n",
       "\n",
       "                                     key_risk_factors  \n",
       "0   [inconsistent ESG strategy, different interpre...  \n",
       "1   [inconsistency in language used in internal do...  \n",
       "2   [inconsistent expectations in environmental cr...  \n",
       "3   [engine failure, grounding, irregular vibratio...  \n",
       "4   [driver error, vehicle maintenance, sensor cal...  \n",
       "5   [faulty fire alarm panel, unusual smell, contr...  \n",
       "6   [bodily injury, property damage, liability, fa...  \n",
       "7   [exclusions, expected losses, expected or inte...  \n",
       "8   [malicious attacks, accidental system failures...  \n",
       "9   [discrepancies in reported earnings, late enro...  \n",
       "10  [miscalculations of replacement cost estimates...  \n",
       "11  [damage from a broad range of causes, exclusio...  \n",
       "12  [medical emergencies, trip cancellations, tran...  \n",
       "13  [accidental medical emergencies, communication...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example merged risk_summary for the first document:\n",
      "\n",
      "The company's ESG strategy is still developing and not consistently implemented across business units, leading to an inconsistent picture when presenting a single global ESG narrative. Different regions have their own interpretations of sustainability, and the level of documentation varies. The company has attempted to quantify environmental impacts, but the numbers are difficult to compare due to changing boundaries and assumptions. The company's operational risk is high due to inconsistent data collection, vague details on recycling, and estimated waste reduction. The lack of formal policy on waste reduction and inconsistent reporting on social issues also contribute to the risk. Additionally, incomplete supporting documents for safety metrics and year to date estimates for safety metric ...\n",
      "\n",
      "Saved document level CSV to: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\processed\\docs_extracted.csv\n",
      "Saved document level JSON to: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\processed\\docs_extracted.json\n"
     ]
    }
   ],
   "source": [
    "## Step 10: Reduce phase from chunk level to document level\n",
    "\n",
    "from src.reducer import reduce_extractions_for_document\n",
    "\n",
    "def reduce_document_group(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Reduce all chunk level extractions for a single document into one row.\n",
    "    This wraps the reduce_extractions_for_document helper and adds metadata.\n",
    "    \"\"\"\n",
    "    # Collect all valid dicts from the extracted_json column\n",
    "    chunk_dicts = [\n",
    "        d for d in group[\"extracted_json\"]\n",
    "        if isinstance(d, dict)\n",
    "    ]\n",
    "\n",
    "    # Build simple metadata for this document\n",
    "    doc_meta = {\n",
    "        \"doc_index\": int(group[\"doc_index\"].iloc[0]),\n",
    "        \"category\": group[\"category\"].iloc[0],\n",
    "        \"filename\": group[\"filename\"].iloc[0],\n",
    "        \"n_chunks\": len(group),\n",
    "        \"n_chunks_with_data\": len(chunk_dicts),\n",
    "    }\n",
    "\n",
    "    # Call the reducer helper to merge all chunk level dicts\n",
    "    doc_level = reduce_extractions_for_document(\n",
    "        chunk_extractions=chunk_dicts,\n",
    "        doc_metadata=doc_meta,\n",
    "    )\n",
    "\n",
    "    # Convert dict to Series so that groupby.apply can stack them\n",
    "    return pd.Series(doc_level)\n",
    "\n",
    "\n",
    "# Apply the reducer to every document in chunk_outputs_df\n",
    "docs_extracted_df = (\n",
    "    chunk_outputs_df\n",
    "    .groupby(\"doc_index\", group_keys=False)\n",
    "    .apply(reduce_document_group)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Document level extraction overview:\")\n",
    "display(\n",
    "    docs_extracted_df[\n",
    "        [\n",
    "            \"doc_index\",\n",
    "            \"category\",\n",
    "            \"filename\",\n",
    "            \"n_chunks\",\n",
    "            \"n_chunks_with_data\",\n",
    "            \"entity_name\",\n",
    "            \"region\",\n",
    "            \"sector\",\n",
    "            \"risk_type\",\n",
    "            \"time_horizon\",\n",
    "            \"key_risk_factors\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nExample merged risk_summary for the first document:\\n\")\n",
    "print(docs_extracted_df.loc[0, \"risk_summary\"])\n",
    "\n",
    "\n",
    "# Optional: save document level outputs to data/processed for later notebooks\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_csv = DATA_PROCESSED_DIR / \"docs_extracted.csv\"\n",
    "output_json = DATA_PROCESSED_DIR / \"docs_extracted.json\"\n",
    "\n",
    "docs_extracted_df.to_csv(output_csv, index=False)\n",
    "docs_extracted_df.to_json(output_json, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"\\nSaved document level CSV to: {output_csv}\")\n",
    "print(f\"Saved document level JSON to: {output_json}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f282c",
   "metadata": {},
   "source": [
    "## Final Summary and Next Steps\n",
    "\n",
    "Notebook 01 is now complete. In this notebook we built an end to end GenAI extraction pipeline that prepares unstructured insurance style documents for downstream analysis. The workflow followed a clear MapReduce pattern:\n",
    "\n",
    "1. Load and inspect all raw documents.  \n",
    "2. Apply a consistent sliding window chunking strategy.  \n",
    "3. Define a strict JSON schema with controlled vocabularies.  \n",
    "4. Build a stable prompt template for structured extraction.  \n",
    "5. Run the Map phase by calling the LLM on every chunk.  \n",
    "6. Validate outputs and collect chunk level results.  \n",
    "7. Run the Reduce phase to combine chunk level dicts into one document level record.  \n",
    "8. Save the final merged dataset to `data/processed` for EDA and modelling.\n",
    "\n",
    "This gives us a clean table with one row per document and consistent fields such as entity name, region, sector, risk type, and extracted risk factors. These outputs form the foundation for Notebook 02, where we will explore the distributions of the extracted fields, inspect missingness patterns, measure text lengths, and begin building domain informed features.\n",
    "\n",
    "The next notebook will focus on EDA and normalisation, and will help us understand how the extracted signals vary across policy documents, ESG reports, and incident summaries. This mirrors the exploratory work an insurtech team would perform before building any risk scoring or classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7e417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI Insurance Project",
   "language": "python",
   "name": "genai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
