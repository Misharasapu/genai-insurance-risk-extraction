{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e68fc4",
   "metadata": {},
   "source": [
    "# Notebook 01: GenAI Extraction and Document Chunking\n",
    "\n",
    "This notebook starts the first stage of the end to end GenAI insurance document pipeline. The goal is to take a collection of long, unstructured documents and convert them into structured JSON outputs that can be analysed in later notebooks.\n",
    "\n",
    "The raw dataset includes three document types stored in the data/raw folder:  \n",
    "• Insurance policy wording style documents  \n",
    "• ESG style sustainability reports  \n",
    "• Incident and loss investigation summaries  \n",
    "\n",
    "Each document is synthetic, created through paraphrasing or original generation so that it mimics the style, tone, and complexity of real insurance documents without raising copyright issues. The documents are intentionally messy, unstructured, and varied in length to reflect the challenges seen in real underwriting and claims workflows.\n",
    "\n",
    "Insurance related documents are often long and difficult to process. Important details such as entities, regions, risk factors, exclusions, operational failures, and ESG concerns are spread across multiple paragraphs. Manual extraction is slow and inconsistent. This notebook focuses on building a reproducible GenAI workflow that can extract structured fields in a reliable way.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Load the raw documents from data/raw  \n",
    "2. Split each document into smaller chunks for safer extraction  \n",
    "3. Define a consistent JSON schema for the key fields we want to extract  \n",
    "4. Build a clear prompt template that enforces structure and avoids hallucination  \n",
    "5. Run the extraction process using helper functions in src  \n",
    "6. Apply a simple MapReduce style pattern by merging chunk outputs  \n",
    "7. Validate and save the final JSON outputs into data/processed  \n",
    "\n",
    "The goal is to produce clean, validated, and consistent structured data. These outputs will then be used in Notebook 02 for EDA and in Notebook 03 for feature engineering and classification. This mirrors practical workflows in insurtech companies, where large volumes of unstructured documents must be prepared and structured before modelling or decision support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e4f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
