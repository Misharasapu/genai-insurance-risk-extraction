{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e68fc4",
   "metadata": {},
   "source": [
    "# Notebook 01: GenAI Extraction and Document Chunking\n",
    "\n",
    "This notebook starts the first stage of the end to end GenAI insurance document pipeline. The goal is to take a collection of long, unstructured documents and convert them into structured JSON outputs that can be analysed in later notebooks.\n",
    "\n",
    "The raw dataset includes three document types stored in the data/raw folder:  \n",
    "• Insurance policy wording style documents  \n",
    "• ESG style sustainability reports  \n",
    "• Incident and loss investigation summaries  \n",
    "\n",
    "Each document is synthetic, created through paraphrasing or original generation so that it mimics the style, tone, and complexity of real insurance documents without raising copyright issues. The documents are intentionally messy, unstructured, and varied in length to reflect the challenges seen in real underwriting and claims workflows.\n",
    "\n",
    "Insurance related documents are often long and difficult to process. Important details such as entities, regions, risk factors, exclusions, operational failures, and ESG concerns are spread across multiple paragraphs. Manual extraction is slow and inconsistent. This notebook focuses on building a reproducible GenAI workflow that can extract structured fields in a reliable way.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Load the raw documents from data/raw  \n",
    "2. Split each document into smaller chunks for safer extraction  \n",
    "3. Define a consistent JSON schema for the key fields we want to extract  \n",
    "4. Build a clear prompt template that enforces structure and avoids hallucination  \n",
    "5. Run the extraction process using helper functions in src  \n",
    "6. Apply a simple MapReduce style pattern by merging chunk outputs  \n",
    "7. Validate and save the final JSON outputs into data/processed  \n",
    "\n",
    "The goal is to produce clean, validated, and consistent structured data. These outputs will then be used in Notebook 02 for EDA and in Notebook 03 for feature engineering and classification. This mirrors practical workflows in insurtech companies, where large volumes of unstructured documents must be prepared and structured before modelling or decision support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07aa14",
   "metadata": {},
   "source": [
    "## Step 1: Set up imports and locate the raw documents\n",
    "\n",
    "Before we can do any chunking or GenAI extraction, we need a reliable way to point the notebook at the `data/raw` folder and see exactly which files we are working with.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Import a small set of core Python utilities  \n",
    "- Define a `PROJECT_ROOT` and `DATA_RAW_DIR` using `pathlib.Path` so paths are robust  \n",
    "- List all text files in the three raw subfolders: `policies`, `esg`, and `incidents`  \n",
    "- Store these file paths in a simple dictionary for later steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\n",
      "Raw data directory: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\raw\n",
      "\n",
      "Discovered raw documents:\n",
      "- esg: 3 files\n",
      "  • esg_corporate_sustainability.txt\n",
      "  • esg_energy_transition.txt\n",
      "  • esg_supply_chain_governance.txt\n",
      "- incident: 3 files\n",
      "  • incident_marine_grounding.txt\n",
      "  • incident_motor_fleet_collision.txt\n",
      "  • incident_property_fire.txt\n",
      "- policy: 8 files\n",
      "  • auto_insurance_policy_synthetic.txt\n",
      "  • businessowners_insurance_synthetic_01.txt\n",
      "  • cyber_insurance_policy_synthetic.txt\n",
      "  • group_life_policy_practice_unstructured_v1.txt\n",
      "  • homeowners_declarations_synthetic.txt\n",
      "  • homeowners_policy_ho3_synthetic.txt\n",
      "  • travel_insurance_policy_synthetic_allianz.txt\n",
      "  • travel_insurance_policy_synthetic_CHI.txt\n",
      "\n",
      "Total number of documents: 14\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1) Define project root and raw data directory\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {DATA_RAW_DIR}\")\n",
    "\n",
    "# 2) Collect documents by category (policies, esg, incidents)\n",
    "documents_by_category = {}\n",
    "\n",
    "for category_dir in sorted(DATA_RAW_DIR.iterdir()):\n",
    "    if category_dir.is_dir():\n",
    "        txt_files = sorted(category_dir.glob(\"*.txt\"))\n",
    "        documents_by_category[category_dir.name] = txt_files\n",
    "\n",
    "# 3) Print a short summary of what we found\n",
    "print(\"\\nDiscovered raw documents:\")\n",
    "for category, paths in documents_by_category.items():\n",
    "    print(f\"- {category}: {len(paths)} files\")\n",
    "    for path in paths:\n",
    "        print(f\"  • {path.name}\")\n",
    "\n",
    "# 4) Optional: a flat list of all documents (for later steps if needed)\n",
    "all_documents = [p for paths in documents_by_category.values() for p in paths]\n",
    "print(f\"\\nTotal number of documents: {len(all_documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff592c95",
   "metadata": {},
   "source": [
    "## Step 2: Load document contents and create a simple overview\n",
    "\n",
    "Now that we know where the files are and how many we have in each category, the next step is to actually **load the text contents** into memory and build a simple summary table.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Read each `.txt` file into a Python string  \n",
    "- Capture basic metadata for each document  \n",
    "  - category (policies, esg, incidents)  \n",
    "  - filename  \n",
    "  - full filesystem path  \n",
    "  - character count  \n",
    "  - word count  \n",
    "  - a short text preview  \n",
    "- Store everything in a pandas `DataFrame` so we can inspect the documents in a structured way\n",
    "\n",
    "\n",
    "This overview will help us:\n",
    "\n",
    "- Decide sensible chunk sizes later  \n",
    "- Check that the documents have realistic lengths  \n",
    "- Quickly spot any weird or empty files before we start chunking and extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aebc3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>714</td>\n",
       "      <td>4958</td>\n",
       "      <td>Synthetic ESG Report – Corporate Sustainabilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>847</td>\n",
       "      <td>5874</td>\n",
       "      <td>Synthetic ESG Report – Energy Transition and E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>823</td>\n",
       "      <td>5755</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>778</td>\n",
       "      <td>4941</td>\n",
       "      <td>Synthetic Marine Incident Report – Engine Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_motor_fleet_collision.txt</td>\n",
       "      <td>788</td>\n",
       "      <td>5051</td>\n",
       "      <td>Synthetic Incident Report – Motor Fleet Collis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_property_fire.txt</td>\n",
       "      <td>789</td>\n",
       "      <td>5086</td>\n",
       "      <td>Synthetic Incident Report – Commercial Propert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>1025</td>\n",
       "      <td>6981</td>\n",
       "      <td>Synthetic Auto Insurance Practice Document  (f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>policy</td>\n",
       "      <td>businessowners_insurance_synthetic_01.txt</td>\n",
       "      <td>1896</td>\n",
       "      <td>12083</td>\n",
       "      <td>Synthetic Businessowners Insurance Practice Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>policy</td>\n",
       "      <td>cyber_insurance_policy_synthetic.txt</td>\n",
       "      <td>1089</td>\n",
       "      <td>7425</td>\n",
       "      <td>Synthetic Cyber Insurance Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>policy</td>\n",
       "      <td>group_life_policy_practice_unstructured_v1.txt</td>\n",
       "      <td>1376</td>\n",
       "      <td>8881</td>\n",
       "      <td>Synthetic Group Life Insurance Practice Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_declarations_synthetic.txt</td>\n",
       "      <td>1055</td>\n",
       "      <td>7149</td>\n",
       "      <td>Synthetic Homeowners Declarations Page Practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_policy_ho3_synthetic.txt</td>\n",
       "      <td>907</td>\n",
       "      <td>6145</td>\n",
       "      <td>Synthetic Homeowners HO-3 Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_allianz.txt</td>\n",
       "      <td>945</td>\n",
       "      <td>6357</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_CHI.txt</td>\n",
       "      <td>859</td>\n",
       "      <td>5838</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                        filename  n_words  \\\n",
       "0        esg                esg_corporate_sustainability.txt      714   \n",
       "1        esg                       esg_energy_transition.txt      847   \n",
       "2        esg                 esg_supply_chain_governance.txt      823   \n",
       "3   incident                   incident_marine_grounding.txt      778   \n",
       "4   incident              incident_motor_fleet_collision.txt      788   \n",
       "5   incident                      incident_property_fire.txt      789   \n",
       "6     policy             auto_insurance_policy_synthetic.txt     1025   \n",
       "7     policy       businessowners_insurance_synthetic_01.txt     1896   \n",
       "8     policy            cyber_insurance_policy_synthetic.txt     1089   \n",
       "9     policy  group_life_policy_practice_unstructured_v1.txt     1376   \n",
       "10    policy           homeowners_declarations_synthetic.txt     1055   \n",
       "11    policy             homeowners_policy_ho3_synthetic.txt      907   \n",
       "12    policy   travel_insurance_policy_synthetic_allianz.txt      945   \n",
       "13    policy       travel_insurance_policy_synthetic_CHI.txt      859   \n",
       "\n",
       "    n_chars                                            preview  \n",
       "0      4958  Synthetic ESG Report – Corporate Sustainabilit...  \n",
       "1      5874  Synthetic ESG Report – Energy Transition and E...  \n",
       "2      5755  Synthetic ESG Report – Supply Chain and Climat...  \n",
       "3      4941  Synthetic Marine Incident Report – Engine Fail...  \n",
       "4      5051  Synthetic Incident Report – Motor Fleet Collis...  \n",
       "5      5086  Synthetic Incident Report – Commercial Propert...  \n",
       "6      6981  Synthetic Auto Insurance Practice Document  (f...  \n",
       "7     12083  Synthetic Businessowners Insurance Practice Do...  \n",
       "8      7425  Synthetic Cyber Insurance Practice Document  (...  \n",
       "9      8881  Synthetic Group Life Insurance Practice Docume...  \n",
       "10     7149  Synthetic Homeowners Declarations Page Practic...  \n",
       "11     6145  Synthetic Homeowners HO-3 Practice Document  (...  \n",
       "12     6357  Synthetic Travel Insurance Practice Document  ...  \n",
       "13     5838  Synthetic Travel Insurance Practice Document (...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load contents of each document into memory and collect basic stats\n",
    "records = []\n",
    "\n",
    "for category, paths in documents_by_category.items():\n",
    "    for path in paths:\n",
    "        # Read the full text of the file\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        # Build a short, single line preview for quick inspection\n",
    "        preview = text[:400].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        # Append a record (one row) for this document\n",
    "        records.append(\n",
    "            {\n",
    "                \"category\": category,\n",
    "                \"filename\": path.name,\n",
    "                \"path\": path,\n",
    "                \"n_chars\": len(text),\n",
    "                \"n_words\": len(text.split()),\n",
    "                \"preview\": preview,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# 2) Create a DataFrame with one row per document\n",
    "docs_df = pd.DataFrame(records)\n",
    "\n",
    "# 3) Show a compact summary\n",
    "print(\"Document overview:\")\n",
    "display(\n",
    "    docs_df[[\"category\", \"filename\", \"n_words\", \"n_chars\", \"preview\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7e5cd",
   "metadata": {},
   "source": [
    "## Step 3: Define a text chunking helper function\n",
    "\n",
    "The next step is to define a consistent strategy for splitting each document into smaller chunks that can be sent to the LLM.\n",
    "\n",
    "In this project we will:\n",
    "\n",
    "- Chunk by **word count** rather than characters so that chunk sizes are easier to interpret.\n",
    "- Use fixed size windows with a **small overlap** so that important sentences near boundaries are not lost.\n",
    "- Keep the function parameters flexible so that chunk sizes can be tuned later without changing the rest of the pipeline.\n",
    "\n",
    "Design choices:\n",
    "\n",
    "- `max_words = 250`: target size of each chunk.\n",
    "- `overlap = 50`: number of words that overlap between consecutive chunks.\n",
    "- Output: a list of chunks, where each chunk is a string.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Implement a `chunk_text` helper function.\n",
    "2. Apply it to one example document from `docs_df`.\n",
    "3. Inspect the number of chunks and a short preview, to check that the behaviour is sensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd971f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    max_words: int = 250,\n",
    "    overlap: int = 50,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a long text into overlapping chunks based on word count.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The full document text as a single string.\n",
    "    max_words : int, optional\n",
    "        Maximum number of words in each chunk.\n",
    "    overlap : int, optional\n",
    "        Number of words that overlap between consecutive chunks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of text chunks. Each chunk is a string containing\n",
    "        up to max_words words, with overlap between neighbours.\n",
    "    \"\"\"\n",
    "    # Split the text into individual words\n",
    "    words = text.split()\n",
    "\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "\n",
    "    # Safety check on overlap\n",
    "    if overlap >= max_words:\n",
    "        raise ValueError(\"overlap must be smaller than max_words\")\n",
    "\n",
    "    # Slide a window over the list of words\n",
    "    while start < len(words):\n",
    "        end = start + max_words\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text_str = \" \".join(chunk_words).strip()\n",
    "        if chunk_text_str:\n",
    "            chunks.append(chunk_text_str)\n",
    "\n",
    "        # Move the start forward by max_words - overlap\n",
    "        start = start + max_words - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Quick test on a single example document\n",
    "example_row = docs_df.iloc[0]\n",
    "example_text = example_row[\"path\"].read_text(encoding=\"utf-8\")\n",
    "\n",
    "example_chunks = chunk_text(example_text, max_words=250, overlap=50)\n",
    "\n",
    "print(f\"Example document: {example_row['filename']}\")\n",
    "print(f\"Total words in document: {example_row['n_words']}\")\n",
    "print(f\"Number of chunks created: {len(example_chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(example_chunks[:2]):\n",
    "    print(f\"--- Chunk {i} (first 40 words) ---\")\n",
    "    print(\" \".join(chunk.split()[:40]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f8f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
