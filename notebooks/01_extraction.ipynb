{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e68fc4",
   "metadata": {},
   "source": [
    "# Notebook 01: GenAI Extraction and Document Chunking\n",
    "\n",
    "This notebook starts the first stage of the end to end GenAI insurance document pipeline. The goal is to take a collection of long, unstructured documents and convert them into structured JSON outputs that can be analysed in later notebooks.\n",
    "\n",
    "The raw dataset includes three document types stored in the data/raw folder:  \n",
    "• Insurance policy wording style documents  \n",
    "• ESG style sustainability reports  \n",
    "• Incident and loss investigation summaries  \n",
    "\n",
    "Each document is synthetic, created through paraphrasing or original generation so that it mimics the style, tone, and complexity of real insurance documents without raising copyright issues. The documents are intentionally messy, unstructured, and varied in length to reflect the challenges seen in real underwriting and claims workflows.\n",
    "\n",
    "Insurance related documents are often long and difficult to process. Important details such as entities, regions, risk factors, exclusions, operational failures, and ESG concerns are spread across multiple paragraphs. Manual extraction is slow and inconsistent. This notebook focuses on building a reproducible GenAI workflow that can extract structured fields in a reliable way.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Load the raw documents from data/raw  \n",
    "2. Split each document into smaller chunks for safer extraction  \n",
    "3. Define a consistent JSON schema for the key fields we want to extract  \n",
    "4. Build a clear prompt template that enforces structure and avoids hallucination  \n",
    "5. Run the extraction process using helper functions in src  \n",
    "6. Apply a simple MapReduce style pattern by merging chunk outputs  \n",
    "7. Validate and save the final JSON outputs into data/processed  \n",
    "\n",
    "The goal is to produce clean, validated, and consistent structured data. These outputs will then be used in Notebook 02 for EDA and in Notebook 03 for feature engineering and classification. This mirrors practical workflows in insurtech companies, where large volumes of unstructured documents must be prepared and structured before modelling or decision support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07aa14",
   "metadata": {},
   "source": [
    "## Step 1: Set up imports and locate the raw documents\n",
    "\n",
    "Before we can do any chunking or GenAI extraction, we need a reliable way to point the notebook at the `data/raw` folder and see exactly which files we are working with.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Import a small set of core Python utilities  \n",
    "- Define a `PROJECT_ROOT` and `DATA_RAW_DIR` using `pathlib.Path` so paths are robust  \n",
    "- List all text files in the three raw subfolders: `policies`, `esg`, and `incidents`  \n",
    "- Store these file paths in a simple dictionary for later steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\n",
      "Raw data directory: C:\\Users\\misha\\OneDrive - University of Bristol\\Job Apps\\Concirrus\\genai-insurance-risk-extraction\\data\\raw\n",
      "\n",
      "Discovered raw documents:\n",
      "- esg: 3 files\n",
      "  • esg_corporate_sustainability.txt\n",
      "  • esg_energy_transition.txt\n",
      "  • esg_supply_chain_governance.txt\n",
      "- incident: 3 files\n",
      "  • incident_marine_grounding.txt\n",
      "  • incident_motor_fleet_collision.txt\n",
      "  • incident_property_fire.txt\n",
      "- policy: 8 files\n",
      "  • auto_insurance_policy_synthetic.txt\n",
      "  • businessowners_insurance_synthetic_01.txt\n",
      "  • cyber_insurance_policy_synthetic.txt\n",
      "  • group_life_policy_practice_unstructured_v1.txt\n",
      "  • homeowners_declarations_synthetic.txt\n",
      "  • homeowners_policy_ho3_synthetic.txt\n",
      "  • travel_insurance_policy_synthetic_allianz.txt\n",
      "  • travel_insurance_policy_synthetic_CHI.txt\n",
      "\n",
      "Total number of documents: 14\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1) Define project root and raw data directory\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {DATA_RAW_DIR}\")\n",
    "\n",
    "# 2) Collect documents by category (policies, esg, incidents)\n",
    "documents_by_category = {}\n",
    "\n",
    "for category_dir in sorted(DATA_RAW_DIR.iterdir()):\n",
    "    if category_dir.is_dir():\n",
    "        txt_files = sorted(category_dir.glob(\"*.txt\"))\n",
    "        documents_by_category[category_dir.name] = txt_files\n",
    "\n",
    "# 3) Print a short summary of what we found\n",
    "print(\"\\nDiscovered raw documents:\")\n",
    "for category, paths in documents_by_category.items():\n",
    "    print(f\"- {category}: {len(paths)} files\")\n",
    "    for path in paths:\n",
    "        print(f\"  • {path.name}\")\n",
    "\n",
    "# 4) Optional: a flat list of all documents (for later steps if needed)\n",
    "all_documents = [p for paths in documents_by_category.values() for p in paths]\n",
    "print(f\"\\nTotal number of documents: {len(all_documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0538e15b",
   "metadata": {},
   "source": [
    "## Step 2: Load document contents and create a simple overview\n",
    "\n",
    "Now that we know where the files are and how many we have in each category, the next step is to actually **load the text contents** into memory and build a simple summary table.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "- Read each `.txt` file into a Python string  \n",
    "- Capture basic metadata for each document  \n",
    "  - category (policies, esg, incidents)  \n",
    "  - filename  \n",
    "  - full filesystem path  \n",
    "  - character count  \n",
    "  - word count  \n",
    "  - a short text preview  \n",
    "- Store everything in a pandas `DataFrame` so we can inspect the documents in a structured way\n",
    "\n",
    "\n",
    "This overview will help us:\n",
    "\n",
    "- Decide sensible chunk sizes later  \n",
    "- Check that the documents have realistic lengths  \n",
    "- Quickly spot any weird or empty files before we start chunking and extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baa9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>714</td>\n",
       "      <td>4958</td>\n",
       "      <td>Synthetic ESG Report – Corporate Sustainabilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>847</td>\n",
       "      <td>5874</td>\n",
       "      <td>Synthetic ESG Report – Energy Transition and E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>823</td>\n",
       "      <td>5755</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_marine_grounding.txt</td>\n",
       "      <td>778</td>\n",
       "      <td>4941</td>\n",
       "      <td>Synthetic Marine Incident Report – Engine Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_motor_fleet_collision.txt</td>\n",
       "      <td>788</td>\n",
       "      <td>5051</td>\n",
       "      <td>Synthetic Incident Report – Motor Fleet Collis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incident</td>\n",
       "      <td>incident_property_fire.txt</td>\n",
       "      <td>789</td>\n",
       "      <td>5086</td>\n",
       "      <td>Synthetic Incident Report – Commercial Propert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policy</td>\n",
       "      <td>auto_insurance_policy_synthetic.txt</td>\n",
       "      <td>1025</td>\n",
       "      <td>6981</td>\n",
       "      <td>Synthetic Auto Insurance Practice Document  (f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>policy</td>\n",
       "      <td>businessowners_insurance_synthetic_01.txt</td>\n",
       "      <td>1896</td>\n",
       "      <td>12083</td>\n",
       "      <td>Synthetic Businessowners Insurance Practice Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>policy</td>\n",
       "      <td>cyber_insurance_policy_synthetic.txt</td>\n",
       "      <td>1089</td>\n",
       "      <td>7425</td>\n",
       "      <td>Synthetic Cyber Insurance Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>policy</td>\n",
       "      <td>group_life_policy_practice_unstructured_v1.txt</td>\n",
       "      <td>1376</td>\n",
       "      <td>8881</td>\n",
       "      <td>Synthetic Group Life Insurance Practice Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_declarations_synthetic.txt</td>\n",
       "      <td>1055</td>\n",
       "      <td>7149</td>\n",
       "      <td>Synthetic Homeowners Declarations Page Practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>policy</td>\n",
       "      <td>homeowners_policy_ho3_synthetic.txt</td>\n",
       "      <td>907</td>\n",
       "      <td>6145</td>\n",
       "      <td>Synthetic Homeowners HO-3 Practice Document  (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_allianz.txt</td>\n",
       "      <td>945</td>\n",
       "      <td>6357</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>policy</td>\n",
       "      <td>travel_insurance_policy_synthetic_CHI.txt</td>\n",
       "      <td>859</td>\n",
       "      <td>5838</td>\n",
       "      <td>Synthetic Travel Insurance Practice Document (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                        filename  n_words  \\\n",
       "0        esg                esg_corporate_sustainability.txt      714   \n",
       "1        esg                       esg_energy_transition.txt      847   \n",
       "2        esg                 esg_supply_chain_governance.txt      823   \n",
       "3   incident                   incident_marine_grounding.txt      778   \n",
       "4   incident              incident_motor_fleet_collision.txt      788   \n",
       "5   incident                      incident_property_fire.txt      789   \n",
       "6     policy             auto_insurance_policy_synthetic.txt     1025   \n",
       "7     policy       businessowners_insurance_synthetic_01.txt     1896   \n",
       "8     policy            cyber_insurance_policy_synthetic.txt     1089   \n",
       "9     policy  group_life_policy_practice_unstructured_v1.txt     1376   \n",
       "10    policy           homeowners_declarations_synthetic.txt     1055   \n",
       "11    policy             homeowners_policy_ho3_synthetic.txt      907   \n",
       "12    policy   travel_insurance_policy_synthetic_allianz.txt      945   \n",
       "13    policy       travel_insurance_policy_synthetic_CHI.txt      859   \n",
       "\n",
       "    n_chars                                            preview  \n",
       "0      4958  Synthetic ESG Report – Corporate Sustainabilit...  \n",
       "1      5874  Synthetic ESG Report – Energy Transition and E...  \n",
       "2      5755  Synthetic ESG Report – Supply Chain and Climat...  \n",
       "3      4941  Synthetic Marine Incident Report – Engine Fail...  \n",
       "4      5051  Synthetic Incident Report – Motor Fleet Collis...  \n",
       "5      5086  Synthetic Incident Report – Commercial Propert...  \n",
       "6      6981  Synthetic Auto Insurance Practice Document  (f...  \n",
       "7     12083  Synthetic Businessowners Insurance Practice Do...  \n",
       "8      7425  Synthetic Cyber Insurance Practice Document  (...  \n",
       "9      8881  Synthetic Group Life Insurance Practice Docume...  \n",
       "10     7149  Synthetic Homeowners Declarations Page Practic...  \n",
       "11     6145  Synthetic Homeowners HO-3 Practice Document  (...  \n",
       "12     6357  Synthetic Travel Insurance Practice Document  ...  \n",
       "13     5838  Synthetic Travel Insurance Practice Document (...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load contents of each document into memory and collect basic stats\n",
    "records = []\n",
    "\n",
    "for category, paths in documents_by_category.items():\n",
    "    for path in paths:\n",
    "        # Read the full text of the file\n",
    "        text = path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        # Build a short, single line preview for quick inspection\n",
    "        preview = text[:400].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        # Append a record (one row) for this document\n",
    "        records.append(\n",
    "            {\n",
    "                \"category\": category,\n",
    "                \"filename\": path.name,\n",
    "                \"path\": path,\n",
    "                \"n_chars\": len(text),\n",
    "                \"n_words\": len(text.split()),\n",
    "                \"preview\": preview,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# 2) Create a DataFrame with one row per document\n",
    "docs_df = pd.DataFrame(records)\n",
    "\n",
    "# 3) Show a compact summary\n",
    "print(\"Document overview:\")\n",
    "display(\n",
    "    docs_df[[\"category\", \"filename\", \"n_words\", \"n_chars\", \"preview\"]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e6b13",
   "metadata": {},
   "source": [
    "## Step 3: Apply a consistent text chunking strategy\n",
    "\n",
    "Now that the helper function has been implemented in `src/chunking.py`, we can import it and use it to split each document into manageable chunks for LLM processing.\n",
    "\n",
    "In this project we use a word based chunking strategy with the following design choices:\n",
    "\n",
    "- Split text into chunks of approximately 250 words.\n",
    "- Use an overlap of 50 words so that important details near chunk boundaries are preserved.\n",
    "- Keep the parameters flexible so that chunk sizes can be adjusted later without changing the pipeline.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Import the `chunk_text` helper function from `src/chunking.py`.\n",
    "2. Apply it to a single example document.\n",
    "3. Inspect the number of chunks and preview the first few to confirm that the behaviour is sensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7febddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example document: esg_corporate_sustainability.txt\n",
      "Total words in document: 714\n",
      "Number of chunks created: 4\n",
      "\n",
      "--- Chunk 0 (first 40 words) ---\n",
      "Synthetic ESG Report – Corporate Sustainability Narrative (fully synthetic paraphrased text created for training and GenAI extraction testing; not based on any copyrighted ESG document) (inspired by: corporate ESG and sustainability disclosures from global manufacturing, logistics, and energy companies) The\n",
      "\n",
      "--- Chunk 1 (first 40 words) ---\n",
      "the footnotes said the reductions were influenced by lower production volumes rather than actual efficiency improvements. No single team seemed responsible for consolidating the data, which led to confusion over which version was the most accurate. The company recycled some\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure src/ is on the Python path\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import the helper function\n",
    "from src.chunking import chunk_text\n",
    "\n",
    "# Test on one example document\n",
    "example_row = docs_df.iloc[0]\n",
    "example_text = example_row[\"path\"].read_text(encoding=\"utf-8\")\n",
    "\n",
    "example_chunks = chunk_text(example_text, max_words=250, overlap=50)\n",
    "\n",
    "print(f\"Example document: {example_row['filename']}\")\n",
    "print(f\"Total words in document: {example_row['n_words']}\")\n",
    "print(f\"Number of chunks created: {len(example_chunks)}\\n\")\n",
    "\n",
    "# Preview the first two chunks\n",
    "for i, chunk in enumerate(example_chunks[:2]):\n",
    "    print(f\"--- Chunk {i} (first 40 words) ---\")\n",
    "    print(\" \".join(chunk.split()[:40]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3ddbf",
   "metadata": {},
   "source": [
    "## Step 4: Chunk all documents and build a chunk level table\n",
    "\n",
    "In the previous step we confirmed that the `chunk_text` helper function produces sensible chunks for a single document. The next step is to apply this function to every document in the corpus and create a structured table of chunks.\n",
    "\n",
    "The goal of this step is to move from a **document level view** (`docs_df`) to a **chunk level view** that is suitable for LLM extraction.\n",
    "\n",
    "In this step we will:\n",
    "\n",
    "1. Loop over all rows in `docs_df` and apply `chunk_text` to each document.\n",
    "2. For each chunk, record the following metadata:\n",
    "   - `category` (policies, esg, incidents)\n",
    "   - `filename`\n",
    "   - `doc_index` (index of the document in `docs_df`)\n",
    "   - `chunk_index` (position of the chunk within that document)\n",
    "   - `chunk_text`\n",
    "   - `n_words_chunk` (word count in the chunk)\n",
    "3. Store all chunk records in a new pandas DataFrame called `chunks_df`.\n",
    "\n",
    "The `chunks_df` table will have one row per chunk and will serve as the main input for the LLM extraction step. This makes it easy to track where each chunk came from and to aggregate results back to the document level later in the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7cb820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk level overview:\n",
      "- Number of documents: 14\n",
      "- Total number of chunks: 76\n",
      "- Average chunks per document: 5.43\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>n_words_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_corporate_sustainability.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_energy_transition.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>esg_supply_chain_governance.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index category                          filename  chunk_index  \\\n",
       "0          0      esg  esg_corporate_sustainability.txt            0   \n",
       "1          0      esg  esg_corporate_sustainability.txt            1   \n",
       "2          0      esg  esg_corporate_sustainability.txt            2   \n",
       "3          0      esg  esg_corporate_sustainability.txt            3   \n",
       "4          1      esg         esg_energy_transition.txt            0   \n",
       "5          1      esg         esg_energy_transition.txt            1   \n",
       "6          1      esg         esg_energy_transition.txt            2   \n",
       "7          1      esg         esg_energy_transition.txt            3   \n",
       "8          1      esg         esg_energy_transition.txt            4   \n",
       "9          2      esg   esg_supply_chain_governance.txt            0   \n",
       "\n",
       "   n_words_chunk  \n",
       "0            250  \n",
       "1            250  \n",
       "2            250  \n",
       "3            114  \n",
       "4            250  \n",
       "5            250  \n",
       "6            250  \n",
       "7            247  \n",
       "8             47  \n",
       "9            250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Chunk all documents and build a chunk-level DataFrame\n",
    "\n",
    "chunk_records = []\n",
    "\n",
    "for doc_index, row in docs_df.iterrows():\n",
    "    # Read full text for this document\n",
    "    text = row[\"path\"].read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Generate chunks using the helper\n",
    "    chunks = chunk_text(text, max_words=250, overlap=50)\n",
    "\n",
    "    # Create one record per chunk\n",
    "    for chunk_index, chunk in enumerate(chunks):\n",
    "        chunk_records.append(\n",
    "            {\n",
    "                \"doc_index\": doc_index,\n",
    "                \"category\": row[\"category\"],\n",
    "                \"filename\": row[\"filename\"],\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"chunk_text\": chunk,\n",
    "                \"n_words_chunk\": len(chunk.split()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Build the chunk-level DataFrame\n",
    "chunks_df = pd.DataFrame(chunk_records)\n",
    "\n",
    "print(\"Chunk level overview:\")\n",
    "print(f\"- Number of documents: {len(docs_df)}\")\n",
    "print(f\"- Total number of chunks: {len(chunks_df)}\")\n",
    "print(f\"- Average chunks per document: {len(chunks_df) / len(docs_df):.2f}\\n\")\n",
    "\n",
    "display(\n",
    "    chunks_df[[\"doc_index\", \"category\", \"filename\", \"chunk_index\", \"n_words_chunk\"]]\n",
    "    .head(10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07252e6f",
   "metadata": {},
   "source": [
    "## Step 5: Define the JSON schema and controlled vocabularies\n",
    "\n",
    "Before calling the LLM, we need a clear and consistent definition of the structured output we expect from each chunk. This ensures that:\n",
    "\n",
    "- The model always returns the same fields.\n",
    "- Values are constrained where appropriate.\n",
    "- Validation in later steps is straightforward.\n",
    "\n",
    "In this project, each extraction call is expected to return the following JSON structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"entity_name\": \"\",\n",
    "  \"region\": \"\",\n",
    "  \"sector\": \"\",\n",
    "  \"risk_type\": \"\",\n",
    "  \"time_horizon\": \"\",\n",
    "  \"key_risk_factors\": [],\n",
    "  \"risk_summary\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b75bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
