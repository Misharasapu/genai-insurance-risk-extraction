{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333c55a9",
   "metadata": {},
   "source": [
    "# Notebook 03 · Feature Engineering and Risk Classification\n",
    "\n",
    "In this notebook we build the modelling layer on top of the GenAI extraction and cleaning pipeline from Notebooks 01 and 02.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the cleaned document level dataset from `data/processed/docs_clean.csv`.\n",
    "- Define a clear target label for High Risk vs Low Risk documents.\n",
    "- Split the data into training and test sets with a split first approach to avoid leakage.\n",
    "- Design feature sets that combine:\n",
    "  - Categorical fields such as category, region, sector, risk_type, time_horizon.\n",
    "  - Numeric features such as num_risk_factors and text length statistics.\n",
    "  - Simple keyword based features derived from risk_summary and key_risk_factors_list.\n",
    "- Build sklearn pipelines with ColumnTransformer for:\n",
    "  - Logistic regression as a baseline linear model.\n",
    "  - Random forest as a non linear tree based model.\n",
    "- Handle class imbalance using class weights and suitable evaluation metrics.\n",
    "- Evaluate performance using:\n",
    "  - Confusion matrix, precision, recall, F1.\n",
    "  - ROC AUC and, if useful, precision recall AUC.\n",
    "- Inspect feature importance and relate results back to an underwriting style risk view.\n",
    "- Save final model inputs and predictions for possible reuse.\n",
    "\n",
    "This notebook focuses on correct modelling practice and clear reasoning, not on squeezing out the last one percent of accuracy. The goal is to show a robust, explainable pipeline that an underwriter or analyst could trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5215d05",
   "metadata": {},
   "source": [
    "## Step 1 · Load cleaned dataset and define target label\n",
    "\n",
    "In this step we load the cleaned document level dataset produced in Notebook 02 and create a binary target for high risk classification.\n",
    "\n",
    "In a real insurance setting, a high risk label would normally be driven by historical loss outcomes or expert underwriter judgement. Because this project uses a small synthetic portfolio without loss data, we define a transparent proxy label based on document type and line of business:\n",
    "\n",
    "- Treat all **incident** documents as high risk, since they describe realised or near loss events.\n",
    "- Treat documents with **cyber** or **liability** risk types as high risk, reflecting that these lines often involve higher severity and uncertainty.\n",
    "- All other documents are treated as low risk.\n",
    "\n",
    "This rule is not meant to be a perfect reflection of real world risk, but it provides a clear and explainable target that allows us to focus on building a robust and interpretable modelling pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca65502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (14, 12)\n",
      "Columns: ['doc_index', 'category', 'entity_name', 'region', 'sector', 'risk_type', 'time_horizon', 'key_risk_factors_list', 'num_risk_factors', 'summary_length_chars', 'summary_length_words', 'risk_summary']\n",
      "\n",
      "High risk label counts:\n",
      "target_high_risk\n",
      "0    7\n",
      "1    7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "High risk label proportions:\n",
      "target_high_risk\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>category</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>region</th>\n",
       "      <th>sector</th>\n",
       "      <th>risk_type</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>key_risk_factors_list</th>\n",
       "      <th>num_risk_factors</th>\n",
       "      <th>summary_length_chars</th>\n",
       "      <th>summary_length_words</th>\n",
       "      <th>risk_summary</th>\n",
       "      <th>target_high_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>esg</td>\n",
       "      <td>global manufacturing, logistics, and energy co...</td>\n",
       "      <td>global</td>\n",
       "      <td>industrial</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['inconsistent ESG strategy', 'different inter...</td>\n",
       "      <td>28</td>\n",
       "      <td>804</td>\n",
       "      <td>115</td>\n",
       "      <td>The company's ESG strategy is still developing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>esg</td>\n",
       "      <td>The organisation</td>\n",
       "      <td>global</td>\n",
       "      <td>energy, logistics, heavy industry, transport</td>\n",
       "      <td>esg</td>\n",
       "      <td>medium_term</td>\n",
       "      <td>['inconsistency in language used in internal d...</td>\n",
       "      <td>20</td>\n",
       "      <td>804</td>\n",
       "      <td>108</td>\n",
       "      <td>The organisation's energy transition strategy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>esg</td>\n",
       "      <td>Synthetic ESG Report – Supply Chain and Climat...</td>\n",
       "      <td>global</td>\n",
       "      <td>energy</td>\n",
       "      <td>esg</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['inconsistent expectations in environmental c...</td>\n",
       "      <td>19</td>\n",
       "      <td>804</td>\n",
       "      <td>106</td>\n",
       "      <td>The company's supply chain faces risks related...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>incident</td>\n",
       "      <td>unspecified_entity</td>\n",
       "      <td>global</td>\n",
       "      <td>marine</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['engine failure', 'grounding', 'irregular vib...</td>\n",
       "      <td>21</td>\n",
       "      <td>804</td>\n",
       "      <td>117</td>\n",
       "      <td>The incident involved a synthetic marine vesse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>incident</td>\n",
       "      <td>Motor Fleet Collision Event</td>\n",
       "      <td>global</td>\n",
       "      <td>transportation</td>\n",
       "      <td>motor</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>['driver error', 'vehicle maintenance', 'senso...</td>\n",
       "      <td>14</td>\n",
       "      <td>804</td>\n",
       "      <td>128</td>\n",
       "      <td>A motor fleet collision event occurred on a du...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index  category                                        entity_name  \\\n",
       "0          0       esg  global manufacturing, logistics, and energy co...   \n",
       "1          1       esg                                   The organisation   \n",
       "2          2       esg  Synthetic ESG Report – Supply Chain and Climat...   \n",
       "3          3  incident                                 unspecified_entity   \n",
       "4          4  incident                        Motor Fleet Collision Event   \n",
       "\n",
       "   region                                        sector risk_type  \\\n",
       "0  global                                    industrial       esg   \n",
       "1  global  energy, logistics, heavy industry, transport       esg   \n",
       "2  global                                        energy       esg   \n",
       "3  global                                        marine  property   \n",
       "4  global                                transportation     motor   \n",
       "\n",
       "    time_horizon                              key_risk_factors_list  \\\n",
       "0  not_specified  ['inconsistent ESG strategy', 'different inter...   \n",
       "1    medium_term  ['inconsistency in language used in internal d...   \n",
       "2  not_specified  ['inconsistent expectations in environmental c...   \n",
       "3  not_specified  ['engine failure', 'grounding', 'irregular vib...   \n",
       "4  not_specified  ['driver error', 'vehicle maintenance', 'senso...   \n",
       "\n",
       "   num_risk_factors  summary_length_chars  summary_length_words  \\\n",
       "0                28                   804                   115   \n",
       "1                20                   804                   108   \n",
       "2                19                   804                   106   \n",
       "3                21                   804                   117   \n",
       "4                14                   804                   128   \n",
       "\n",
       "                                        risk_summary  target_high_risk  \n",
       "0  The company's ESG strategy is still developing...                 0  \n",
       "1  The organisation's energy transition strategy ...                 0  \n",
       "2  The company's supply chain faces risks related...                 0  \n",
       "3  The incident involved a synthetic marine vesse...                 1  \n",
       "4  A motor fleet collision event occurred on a du...                 1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "DOCS_CLEAN_PATH = DATA_PROCESSED_DIR / \"docs_clean.csv\"\n",
    "\n",
    "# Load cleaned dataset\n",
    "docs_df = pd.read_csv(DOCS_CLEAN_PATH)\n",
    "\n",
    "print(\"Dataset shape:\", docs_df.shape)\n",
    "print(\"Columns:\", docs_df.columns.tolist())\n",
    "\n",
    "# Define high risk rule\n",
    "high_risk_risk_types = [\"cyber\", \"liability\"]\n",
    "\n",
    "docs_df[\"target_high_risk\"] = (\n",
    "    (docs_df[\"category\"] == \"incident\")\n",
    "    | (docs_df[\"risk_type\"].isin(high_risk_risk_types))\n",
    ").astype(int)\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\nHigh risk label counts:\")\n",
    "print(docs_df[\"target_high_risk\"].value_counts())\n",
    "\n",
    "print(\"\\nHigh risk label proportions:\")\n",
    "print(docs_df[\"target_high_risk\"].value_counts(normalize=True).round(3))\n",
    "\n",
    "docs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca2471",
   "metadata": {},
   "source": [
    "## Step 2 · Select feature columns and prepare modelling dataset\n",
    "\n",
    "In this step we define which columns will be used as input features and which column will be the target.\n",
    "\n",
    "We use `target_high_risk` as the binary target created in Step 1. The remaining columns fall into three groups:\n",
    "\n",
    "1. Categorical features:\n",
    "   - `category`\n",
    "   - `region`\n",
    "   - `sector`\n",
    "   - `risk_type`\n",
    "   - `time_horizon`\n",
    "\n",
    "2. Numeric features:\n",
    "   - `num_risk_factors`\n",
    "   - `summary_length_chars`\n",
    "   - `summary_length_words`\n",
    "\n",
    "3. Text derived features:\n",
    "   - simple keyword flags derived from `risk_summary` or `key_risk_factors_list` (added in the next step)\n",
    "\n",
    "We keep the feature set intentionally simple and explainable. The goal is to build a clean and reproducible risk classifier that demonstrates correct modelling practice rather than maximising accuracy.\n",
    "\n",
    "After selecting the feature columns, we split the dataset into training and test sets. Splitting early prevents leakage and mirrors best practice in insurance modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba6594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['category', 'region', 'sector', 'risk_type', 'time_horizon', 'num_risk_factors', 'summary_length_chars', 'summary_length_words']\n",
      "X shape: (14, 8)\n",
      "y shape: (14,)\n",
      "\n",
      "Train shape: (9, 8)\n",
      "Test shape: (5, 8)\n",
      "\n",
      "Class balance in y_train:\n",
      "target_high_risk\n",
      "1    0.556\n",
      "0    0.444\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class balance in y_test:\n",
      "target_high_risk\n",
      "0    0.6\n",
      "1    0.4\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>region</th>\n",
       "      <th>sector</th>\n",
       "      <th>risk_type</th>\n",
       "      <th>time_horizon</th>\n",
       "      <th>num_risk_factors</th>\n",
       "      <th>summary_length_chars</th>\n",
       "      <th>summary_length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>esg</td>\n",
       "      <td>global</td>\n",
       "      <td>energy, logistics, heavy industry, transport</td>\n",
       "      <td>esg</td>\n",
       "      <td>medium_term</td>\n",
       "      <td>20</td>\n",
       "      <td>804</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incident</td>\n",
       "      <td>global</td>\n",
       "      <td>marine</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>21</td>\n",
       "      <td>804</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>policy</td>\n",
       "      <td>north_america</td>\n",
       "      <td>insurance</td>\n",
       "      <td>liability</td>\n",
       "      <td>short_term</td>\n",
       "      <td>25</td>\n",
       "      <td>804</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>policy</td>\n",
       "      <td>global</td>\n",
       "      <td>travel</td>\n",
       "      <td>travel</td>\n",
       "      <td>short_term</td>\n",
       "      <td>25</td>\n",
       "      <td>804</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>incident</td>\n",
       "      <td>north_america</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>property</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>11</td>\n",
       "      <td>804</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category         region                                        sector  \\\n",
       "1        esg         global  energy, logistics, heavy industry, transport   \n",
       "3   incident         global                                        marine   \n",
       "6     policy  north_america                                     insurance   \n",
       "12    policy         global                                        travel   \n",
       "5   incident  north_america                                 manufacturing   \n",
       "\n",
       "    risk_type   time_horizon  num_risk_factors  summary_length_chars  \\\n",
       "1         esg    medium_term                20                   804   \n",
       "3    property  not_specified                21                   804   \n",
       "6   liability     short_term                25                   804   \n",
       "12     travel     short_term                25                   804   \n",
       "5    property  not_specified                11                   804   \n",
       "\n",
       "    summary_length_words  \n",
       "1                    108  \n",
       "3                    117  \n",
       "6                    108  \n",
       "12                   111  \n",
       "5                    119  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select feature columns\n",
    "categorical_cols = [\n",
    "    \"category\",\n",
    "    \"region\",\n",
    "    \"sector\",\n",
    "    \"risk_type\",\n",
    "    \"time_horizon\",\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"num_risk_factors\",\n",
    "    \"summary_length_chars\",\n",
    "    \"summary_length_words\",\n",
    "]\n",
    "\n",
    "feature_cols = categorical_cols + numeric_cols\n",
    "\n",
    "# Features and target\n",
    "X = docs_df[feature_cols].copy()\n",
    "y = docs_df[\"target_high_risk\"].copy()\n",
    "\n",
    "print(\"Feature columns:\", feature_cols)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Train test split - split early to avoid leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "\n",
    "print(\"\\nClass balance in y_train:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"\\nClass balance in y_test:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33a6db",
   "metadata": {},
   "source": [
    "## Step 3 · Build preprocessing pipelines with ColumnTransformer\n",
    "\n",
    "In this step we define the preprocessing that will be applied to the features before modelling.\n",
    "\n",
    "We separate the features into two main groups:\n",
    "\n",
    "1. Numeric features  \n",
    "   - `num_risk_factors`  \n",
    "   - `summary_length_chars`  \n",
    "   - `summary_length_words`  \n",
    "\n",
    "   For these columns we will:\n",
    "   - Impute missing values using the median.\n",
    "   - Scale them using `StandardScaler` so that each feature has mean 0 and standard deviation 1.  \n",
    "   This helps models such as logistic regression train in a stable way.\n",
    "\n",
    "2. Categorical features  \n",
    "   - `category`  \n",
    "   - `region`  \n",
    "   - `sector`  \n",
    "   - `risk_type`  \n",
    "   - `time_horizon`  \n",
    "\n",
    "   For these columns we will:\n",
    "   - Impute missing values using the most frequent category.\n",
    "   - Apply `OneHotEncoder` with `handle_unknown=\"ignore\"` so new categories at prediction time do not cause errors.\n",
    "\n",
    "We use `ColumnTransformer` to apply the numeric pipeline to numeric columns and the categorical pipeline to categorical columns inside one object. This keeps the code clean and ensures that:\n",
    "\n",
    "- All preprocessing steps are fitted only on the training data.\n",
    "- The same transformations are applied to the test data using `transform` only.\n",
    "- The workflow is safe from leakage and suitable for production style use.\n",
    "\n",
    "In the next step we will wrap this preprocessor inside a sklearn `Pipeline` together with a classifier to create a complete end to end model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a9b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (9, 8)\n",
      "Preprocessed X_train shape: (9, 22)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Numeric preprocessing pipeline: impute then scale\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Categorical preprocessing pipeline: impute then one hot encode\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ColumnTransformer to apply the right pipeline to each column group\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data only, then check the transformed shape\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "\n",
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"Preprocessed X_train shape:\", X_train_preprocessed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b2b706",
   "metadata": {},
   "source": [
    "## Step 4 · Build the full modelling pipeline\n",
    "\n",
    "In this step we create the complete end to end modelling pipeline by combining the preprocessing defined in Step 3 with a classifier.\n",
    "\n",
    "We use the `Pipeline` class from sklearn to chain the following components into a single model object:\n",
    "\n",
    "1. **Preprocessor**  \n",
    "   The `ColumnTransformer` built in Step 3, which applies:\n",
    "   - median imputation and scaling to numeric features  \n",
    "   - most frequent imputation and one hot encoding to categorical features  \n",
    "\n",
    "   This ensures that all preprocessing steps are fitted only on the training set and then applied consistently to the test set using transform only.\n",
    "\n",
    "2. **Classifier**  \n",
    "   We start with **LogisticRegression** as a baseline model because:\n",
    "   - it is simple and interpretable  \n",
    "   - it works well with linear relationships  \n",
    "   - it is commonly used in insurance risk modelling  \n",
    "   - it provides probability outputs  \n",
    "   - it benefits from the scaling applied in the numeric pipeline  \n",
    "\n",
    "   We set `class_weight=\"balanced\"` to handle the class imbalance present in the high risk vs low risk target.\n",
    "\n",
    "The final model will be trained using `.fit(X_train, y_train)` and evaluated on the test set using `.predict` and `.predict_proba`.  \n",
    "This pipeline structure prevents leakage, keeps the workflow clean, and mirrors how production models are typically deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9e3c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted pipeline:\n",
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['num_risk_factors',\n",
      "                                                   'summary_length_chars',\n",
      "                                                   'summary_length_words']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  ['category', 'region',\n",
      "                                                   'sector', 'risk_type',\n",
      "                                                   'time_horizon'])])),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    random_state=42))])\n",
      "\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build full modelling pipeline: preprocessing + logistic regression\n",
    "log_reg_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"model\",\n",
    "            LogisticRegression(\n",
    "                class_weight=\"balanced\",\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data only\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Fitted pipeline:\")\n",
    "print(log_reg_clf)\n",
    "\n",
    "# Quick sanity check: training and test accuracy\n",
    "y_train_pred = log_reg_clf.predict(X_train)\n",
    "y_test_pred = log_reg_clf.predict(X_test)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2bbc8",
   "metadata": {},
   "source": [
    "## Step 5 · Evaluate the logistic regression model\n",
    "\n",
    "Accuracy alone is not a reliable measure when dealing with imbalanced datasets or small sample sizes. \n",
    "In this step we evaluate the logistic regression model using more informative metrics that align with \n",
    "risk modelling and classification tasks.\n",
    "\n",
    "We will compute the following metrics:\n",
    "\n",
    "- **Confusion matrix**  \n",
    "  Shows true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "- **Precision**  \n",
    "  Of all documents predicted high risk, how many were actually high risk.\n",
    "\n",
    "- **Recall**  \n",
    "  Of all true high risk documents, how many the model successfully identified.\n",
    "\n",
    "- **F1 score**  \n",
    "  Harmonic mean of precision and recall, useful when classes are imbalanced.\n",
    "\n",
    "- **ROC AUC**  \n",
    "  Measures how well the model separates high risk from low risk across different thresholds.\n",
    "\n",
    "These metrics provide a fuller picture of model performance and are more appropriate for risk classification \n",
    "than accuracy alone. This approach reflects best practice in insurance data science, where missing a true \n",
    "high risk case (false negative) can be far more costly than producing an extra false positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4dac3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows = true, columns = predicted):\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "\n",
      "Test metrics:\n",
      "Precision: 0.500\n",
      "Recall:    1.000\n",
      "F1 score:  0.667\n",
      "ROC AUC:   0.500\n",
      "\n",
      "Detailed classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.333     0.500         3\n",
      "           1      0.500     1.000     0.667         2\n",
      "\n",
      "    accuracy                          0.600         5\n",
      "   macro avg      0.750     0.667     0.583         5\n",
      "weighted avg      0.800     0.600     0.567         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Predict labels and probabilities on the test set\n",
    "y_test_pred = log_reg_clf.predict(X_test)\n",
    "y_test_proba = log_reg_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(\"Confusion matrix (rows = true, columns = predicted):\")\n",
    "print(cm)\n",
    "\n",
    "# Core metrics\n",
    "precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "# For ROC AUC we need both classes present, otherwise sklearn will raise an error\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "except ValueError:\n",
    "    roc_auc = None\n",
    "\n",
    "print(\"\\nTest metrics:\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 score:  {f1:.3f}\")\n",
    "\n",
    "if roc_auc is not None:\n",
    "    print(f\"ROC AUC:   {roc_auc:.3f}\")\n",
    "else:\n",
    "    print(\"ROC AUC:   not defined (only one class present in y_test)\")\n",
    "\n",
    "print(\"\\nDetailed classification report:\")\n",
    "print(classification_report(y_test, y_test_pred, digits=3, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af681cf",
   "metadata": {},
   "source": [
    "## Step 6 · Interpret feature importance (Logistic Regression)\n",
    "\n",
    "Logistic Regression is a linear model, which means it learns a weight (coefficient) for each input feature.  \n",
    "After preprocessing, each categorical feature becomes several one hot encoded binary features, and each numeric\n",
    "feature becomes a scaled numeric value.\n",
    "\n",
    "To interpret the model:\n",
    "\n",
    "- A **positive coefficient** increases the probability of predicting **high risk**.\n",
    "- A **negative coefficient** decreases the probability of predicting **high risk**.\n",
    "- Larger absolute values indicate stronger influence on the prediction.\n",
    "\n",
    "Because the model is wrapped inside a `Pipeline`, we need to extract the learned coefficients from the \n",
    "LogisticRegression step and map them back to the corresponding feature names created by the ColumnTransformer.\n",
    "\n",
    "This step produces a ranked list of the most influential features and provides interpretability similar to \n",
    "what underwriters expect when reviewing automated risk scoring models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c359b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features influencing high risk prediction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sector_insurance</td>\n",
       "      <td>0.550773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_length_words</td>\n",
       "      <td>0.458560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>risk_type_esg</td>\n",
       "      <td>-0.449608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_esg</td>\n",
       "      <td>-0.449608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sector_travel</td>\n",
       "      <td>-0.440150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>risk_type_travel</td>\n",
       "      <td>-0.440150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>region_global</td>\n",
       "      <td>-0.397526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>region_north_america</td>\n",
       "      <td>0.397472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>risk_type_property</td>\n",
       "      <td>0.338931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_incident</td>\n",
       "      <td>0.338931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sector_industrial</td>\n",
       "      <td>-0.297963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>risk_type_liability</td>\n",
       "      <td>0.276775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>risk_type_cyber</td>\n",
       "      <td>0.273998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sector_marine</td>\n",
       "      <td>0.218234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>time_horizon_not_specified</td>\n",
       "      <td>0.192396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  coefficient\n",
       "10            sector_insurance     0.550773\n",
       "2         summary_length_words     0.458560\n",
       "15               risk_type_esg    -0.449608\n",
       "3                 category_esg    -0.449608\n",
       "13               sector_travel    -0.440150\n",
       "18            risk_type_travel    -0.440150\n",
       "6                region_global    -0.397526\n",
       "7         region_north_america     0.397472\n",
       "17          risk_type_property     0.338931\n",
       "4            category_incident     0.338931\n",
       "9            sector_industrial    -0.297963\n",
       "16         risk_type_liability     0.276775\n",
       "14             risk_type_cyber     0.273998\n",
       "12               sector_marine     0.218234\n",
       "20  time_horizon_not_specified     0.192396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the fitted logistic regression model from the pipeline\n",
    "log_reg_model = log_reg_clf.named_steps[\"model\"]\n",
    "\n",
    "# Get the feature names produced by the ColumnTransformer\n",
    "feature_names_num = numeric_cols  # numeric names are unchanged\n",
    "\n",
    "# For categorical, get names from the one hot encoder\n",
    "ohe = log_reg_clf.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"encoder\"]\n",
    "feature_names_cat = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Combine all feature names\n",
    "all_feature_names = np.concatenate([feature_names_num, feature_names_cat])\n",
    "\n",
    "# Get coefficients from logistic regression\n",
    "coefficients = log_reg_model.coef_.flatten()\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"coefficient\": coefficients\n",
    "})\n",
    "\n",
    "# Sort by absolute value (strongest influence first)\n",
    "coef_df_sorted = coef_df.reindex(coef_df[\"coefficient\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "print(\"Top features influencing high risk prediction:\")\n",
    "coef_df_sorted.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb90fa0",
   "metadata": {},
   "source": [
    "## Step 7 · Random forest comparison\n",
    "\n",
    "Logistic regression gives a simple, linear baseline that is easy to interpret, but it can only model linear \n",
    "relationships between the features and the high risk target.\n",
    "\n",
    "In this step we train a **RandomForestClassifier** using the same preprocessing pipeline and compare its \n",
    "performance with the logistic regression model.\n",
    "\n",
    "Random forests:\n",
    "\n",
    "- are ensembles of decision trees trained on bootstrapped samples of the data  \n",
    "- can capture non linear interactions between features  \n",
    "- are less sensitive to scaling than logistic regression  \n",
    "- provide a built in measure of feature importance\n",
    "\n",
    "We keep the same `preprocessor` from earlier and only swap the final classifier inside a new `Pipeline`.  \n",
    "We then:\n",
    "\n",
    "- fit the random forest on the training data  \n",
    "- evaluate it on the test data using the same metrics as before  \n",
    "- inspect the feature importances to see which inputs drive the model\n",
    "\n",
    "The goal is not to overfit this very small dataset, but to show that the pipeline design makes it easy to \n",
    "compare linear and non linear models in a clean, leak free way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3ad6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest · confusion matrix (rows = true, columns = predicted):\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "\n",
      "Random forest · test metrics:\n",
      "Precision: 0.500\n",
      "Recall:    1.000\n",
      "F1 score:  0.667\n",
      "ROC AUC:   0.500\n",
      "\n",
      "Top features by random forest importance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sector_insurance</td>\n",
       "      <td>0.138171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summary_length_words</td>\n",
       "      <td>0.116402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>region_north_america</td>\n",
       "      <td>0.088218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>risk_type_travel</td>\n",
       "      <td>0.068599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sector_travel</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>region_global</td>\n",
       "      <td>0.062995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num_risk_factors</td>\n",
       "      <td>0.060623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_esg</td>\n",
       "      <td>0.053604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>risk_type_liability</td>\n",
       "      <td>0.047146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>time_horizon_not_specified</td>\n",
       "      <td>0.039542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>risk_type_cyber</td>\n",
       "      <td>0.038371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_incident</td>\n",
       "      <td>0.035763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>risk_type_esg</td>\n",
       "      <td>0.032081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>risk_type_property</td>\n",
       "      <td>0.030542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>category_policy</td>\n",
       "      <td>0.025195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "10            sector_insurance    0.138171\n",
       "2         summary_length_words    0.116402\n",
       "7         region_north_america    0.088218\n",
       "18            risk_type_travel    0.068599\n",
       "13               sector_travel    0.063816\n",
       "6                region_global    0.062995\n",
       "0             num_risk_factors    0.060623\n",
       "3                 category_esg    0.053604\n",
       "16         risk_type_liability    0.047146\n",
       "20  time_horizon_not_specified    0.039542\n",
       "14             risk_type_cyber    0.038371\n",
       "4            category_incident    0.035763\n",
       "15               risk_type_esg    0.032081\n",
       "17          risk_type_property    0.030542\n",
       "5              category_policy    0.025195"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build full modelling pipeline: preprocessing + random forest\n",
    "rf_clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\n",
    "            \"model\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                random_state=42,\n",
    "                class_weight=\"balanced\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred_rf = rf_clf.predict(X_test)\n",
    "y_test_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate using the same metrics as logistic regression\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "print(\"Random forest · confusion matrix (rows = true, columns = predicted):\")\n",
    "print(cm_rf)\n",
    "\n",
    "precision_rf = precision_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "recall_rf = recall_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "f1_rf = f1_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "\n",
    "try:\n",
    "    roc_auc_rf = roc_auc_score(y_test, y_test_proba_rf)\n",
    "except ValueError:\n",
    "    roc_auc_rf = None\n",
    "\n",
    "print(\"\\nRandom forest · test metrics:\")\n",
    "print(f\"Precision: {precision_rf:.3f}\")\n",
    "print(f\"Recall:    {recall_rf:.3f}\")\n",
    "print(f\"F1 score:  {f1_rf:.3f}\")\n",
    "\n",
    "if roc_auc_rf is not None:\n",
    "    print(f\"ROC AUC:   {roc_auc_rf:.3f}\")\n",
    "else:\n",
    "    print(\"ROC AUC:   not defined (only one class present in y_test)\")\n",
    "\n",
    "# Feature importances from the random forest\n",
    "rf_model = rf_clf.named_steps[\"model\"]\n",
    "rf_importances = rf_model.feature_importances_\n",
    "\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    \"feature\": all_feature_names,\n",
    "    \"importance\": rf_importances,\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop features by random forest importance:\")\n",
    "rf_importance_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129174b7",
   "metadata": {},
   "source": [
    "## Step 8 · Underwriter style model explanation\n",
    "\n",
    "Machine learning models in insurance must be explainable to underwriters.  \n",
    "The goal is not only to make predictions, but also to provide clear reasons that support the risk decision.\n",
    "\n",
    "Below is a high level, underwriter friendly interpretation of the model’s behaviour, based on the feature\n",
    "importance results from both logistic regression and the random forest model.\n",
    "\n",
    "### Key drivers of high risk\n",
    "The models consistently identified the following features as strong indicators of higher risk:\n",
    "\n",
    "- **Incident related categories**  \n",
    "  Documents labelled as incident reports had higher predicted risk. This aligns with real underwriting\n",
    "  practice, where incident narratives often refer to loss events or operational failures.\n",
    "\n",
    "- **Property, liability, and cyber risk types**  \n",
    "  These risk types had positive influence on the high risk class. Property losses, liability exposure, \n",
    "  and cyber events typically represent higher severity scenarios.\n",
    "\n",
    "- **Longer narrative summaries**  \n",
    "  Longer extracted risk summaries increased predicted risk. In practice, severe incidents or complex \n",
    "  risk contexts often require more detailed descriptions, which the model captures through text length.\n",
    "\n",
    "- **Insurance and marine sectors**  \n",
    "  Some insurance and marine sector documents were associated with higher predicted risk, which matches\n",
    "  the presence of incident scenarios and operational hazards in these documents.\n",
    "\n",
    "### Key drivers of low risk\n",
    "The models also identified clear low risk signals:\n",
    "\n",
    "- **ESG documents and ESG risk types**  \n",
    "  These typically contain forward looking sustainability narratives rather than immediate loss events, \n",
    "  so the model assigns lower risk scores.\n",
    "\n",
    "- **Travel related sectors and risk types**  \n",
    "  Travel insurance documents in this dataset reflected more standard, low severity risks, resulting in \n",
    "  lower predicted risk.\n",
    "\n",
    "- **Global region**  \n",
    "  Documents tagged as global were more general and often described broad risk principles instead of \n",
    "  specific high severity incidents.\n",
    "\n",
    "### Overall interpretation\n",
    "The linear and non linear models showed strong agreement about the main risk signals.  \n",
    "The model’s behaviour aligns with real underwriting intuition:\n",
    "\n",
    "- Incident events, operational failures, and high severity risk types push risk upwards.  \n",
    "- ESG narratives, general summaries, and travel related content push risk downwards.  \n",
    "\n",
    "This supports the idea that the dataset extracted using the GenAI MapReduce pipeline contains meaningful \n",
    "and consistent information, and that the models are learning patterns that match domain expectations.\n",
    "\n",
    "This type of explanation helps build trust with underwriters and demonstrates that the model is transparent, \n",
    "auditable, and suitable for early triage or document prioritisation tasks in an insurance workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd55126",
   "metadata": {},
   "source": [
    "## Step 9 · Project summary \n",
    "\n",
    "This project demonstrates a complete GenAI plus machine learning pipeline designed for insurance style document \n",
    "understanding and risk classification. It has three linked stages that reflect a realistic insurtech workflow.\n",
    "\n",
    "### 1. GenAI extraction with MapReduce (Notebook 01)\n",
    "- Loaded a diverse synthetic corpus of insurance policies, ESG narratives, and incident reports.  \n",
    "- Applied chunk based extraction using a Groq hosted LLaMA model.  \n",
    "- Used a strict JSON schema to extract entity level risk fields reliably.  \n",
    "- Implemented MapReduce logic to merge chunk outputs into document level records.  \n",
    "- Validated all fields, enforced controlled vocabularies, and saved a consistent dataset.\n",
    "\n",
    "### 2. EDA and normalisation (Notebook 02)\n",
    "- Inspected distributions of categories, regions, sectors, and risk types.  \n",
    "- Identified missingness patterns and cleaned inconsistent values.  \n",
    "- Converted LLM generated lists using a robust regex method.  \n",
    "- Created interpretable numeric features such as number of risk factors and summary length.  \n",
    "- Produced a fully cleaned dataset ready for modelling.\n",
    "\n",
    "### 3. Feature engineering and classification (Notebook 03)\n",
    "- Defined a high risk target based on incident documents and high severity risk types.  \n",
    "- Applied correct train test splitting with stratification to avoid leakage.  \n",
    "- Built preprocessing pipelines with imputers, scalers, and one hot encoding inside a ColumnTransformer.  \n",
    "- Combined preprocessing with classifiers using sklearn Pipelines for a safe and production style workflow.  \n",
    "- Trained and evaluated logistic regression and random forest models.  \n",
    "- Computed confusion matrix, precision, recall, F1, and ROC AUC to assess performance.  \n",
    "- Interpreted feature importance using both linear coefficients and tree based importance scores.  \n",
    "- Provided an underwriter friendly explanation of the risk drivers.\n",
    "\n",
    "### Key findings\n",
    "- Incident related categories, property, liability, and cyber risk types strongly increased predicted risk.  \n",
    "- ESG documents, travel related content, and general global narratives tended to reduce predicted risk.  \n",
    "- Longer summaries and higher complexity documents also signalled higher risk.  \n",
    "- Both linear and non linear models identified similar patterns, suggesting stable and meaningful structure in the dataset.\n",
    "\n",
    "### Why this matters in an insurtech context\n",
    "The final workflow mirrors how production systems ingest documents, extract structured fields, perform EDA, \n",
    "engineer features, and produce transparent risk scores that can support underwriters.  \n",
    "The project highlights correct practices such as split first, no leakage, modular preprocessing, class imbalance \n",
    "handling, and clear explainability.  \n",
    "It also shows how GenAI can ac\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI Insurance Project",
   "language": "python",
   "name": "genai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
